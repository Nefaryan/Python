{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38qWvXyoEh4"
      },
      "source": [
        "# Iniziamo a vedere come creare un albero decisionale\n",
        "Nel nostro esempio, una persona proverà a decidere se andare a vedere uno spettacolo comico oppure no.\n",
        "\n",
        "Fortunatamente la nostra persona di esempio è molto meticolosa, ha infatti catalogato ogni volta che c'era uno spettacolo comico in città registrando alcune informazioni sul comico e sulla sua presenza allo spettacolo:\n",
        "\n",
        "<table class=\"ws-table-all co2cars\">\n",
        "<tbody><tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Età</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Esperienza</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Rank</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Nazionalità</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Presenza</font></font></td>\n",
        "</tr><tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">36</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">10</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">9</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">UK</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">42</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">12</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">4</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Stati Uniti d'America</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">23</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">4</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">6</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">N</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">52</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">4</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">4</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Stati Uniti d'America</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">43</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">21</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">8</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Stati Uniti d'America</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">44</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">14</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">5</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">UK</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">66</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">3</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">7</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">N</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">35</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">14</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">9</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">UK</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">52</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">13</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">7</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">N</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">35</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">5</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">9</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">N</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">24</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">3</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">5</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Stati Uniti d'America</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">NO</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">18</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">3</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">7</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">UK</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">45</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">9</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">9</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">UK</font></font></td>\n",
        "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">SÌ</font></font></td>\n",
        "</tr>\n",
        "</tbody></table>\n",
        "\n",
        "Sulla base di questi dati, andremo a creare un albero decisionale in Python che può essere utilizzato per decidere se vale la pena partecipare a nuovi spettacoli.\n",
        "\n",
        "## Da cosa partiamo?\n",
        "\n",
        "Innanzitutto, leggiamo il set di dati con pandas:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas\n",
        "\n",
        "df = pandas.read_csv(\"dati/dataset/spettacoli.csv\")\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per semplicità convertiamo tutti i dati in numerici, le colonne non numeriche sono 'Nazionalità' e 'Go'.\n",
        "\n",
        "Pandas ha il metodo map() che prende un dizionario con informazioni su come convertire i valori.\n",
        "\n",
        "{'UK': 0, 'USA': 1, 'N': 2}\n",
        "\n",
        "Significa convertire i valori \"UK\" in 0, \"USA\" in 1 e \"N\" in 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = {'UK': 0, 'USA': 1, 'N': 2}\n",
        "df['Nationality'] = df['Nationality'].map(d)\n",
        "d = {'YES': 1, 'NO': 0}\n",
        "df['Go'] = df['Go'].map(d)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quindi dobbiamo separare le colonne delle caratteristiche dalla colonna target.\n",
        "\n",
        "Le colonne delle caratteristiche sono le colonne da cui proviamo a fare la previsione e la colonna target è la colonna con i valori che proviamo a prevedere.\n",
        "\n",
        "X contiene le colonne delle caratteristiche, y è la colonna target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Age', 'Experience', 'Rank', 'Nationality']\n",
        "\n",
        "X = df[features]\n",
        "y = df['Go']\n",
        "\n",
        "print(X)\n",
        "print(\"-\"*40)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ora andiamo a importare le librerie e creare effettivamente l'albero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X, y)\n",
        "\n",
        "tree.plot_tree(dtree, feature_names=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ora andiamo a capire passo passo il grafico\n",
        "\n",
        "\n",
        "- Rank <= 6.5 significa ogni comico con un rango pari o inferiore a 6,5 ​​seguirà la True freccia (a sinistra) e gli altri seguiranno la False freccia (a destra).\n",
        "\n",
        "- gini = 0.497 si riferisce alla qualità della divisione ed è sempre un numero compreso tra 0,0 e 0,5, dove 0,0 significherebbe che tutti i campioni hanno ottenuto lo stesso risultato e 0,5 significherebbe che la divisione viene eseguita esattamente a metà.\n",
        "\n",
        "- samples = 13 significa che a questo punto della decisione sono rimasti 13 comici, ovvero tutti poiché questo è il primo passo.\n",
        "\n",
        "- value = [6, 7] significa che di questi 13 comici, 6 riceveranno un \"NO\" e 7 un \"VAI\".\n",
        "\n",
        "### Gini\n",
        "Esistono molti modi per dividere i campioni, in questo tutorial utilizziamo il metodo GINI.\n",
        "\n",
        "Il metodo Gini utilizza questa formula:\n",
        "\n",
        "Gini = 1 - (x/n)2 - (y/n)2\n",
        "\n",
        "Dove x è il numero di risposte positive (\"GO\"), n è il numero di campioni ed y è il numero di risposte negative (\"NO\"), che ci fornisce questo calcolo:\n",
        "\n",
        "1 - (7 / 13)2 - (6 / 13)2 = 0.497\n",
        "\n",
        "### Andiamo avanti nell'analisi del grafico \n",
        "\n",
        "Il passaggio successivo contiene due caselle, una per i comici con un \"Rank\" pari a 6,5 ​​o inferiore e una casella con il resto.\n",
        "\n",
        "#### Vero - 5 comici finiscono qui:\n",
        "- gini = 0.0 significa che tutti i campioni hanno ottenuto lo stesso risultato.\n",
        "\n",
        "- samples = 5 significa che sono rimasti 5 comici in questo ramo (5 comici con un grado 6,5 o inferiore).\n",
        "\n",
        "- value = [5, 0] significa che 5 comici otterranno un \"NO\" (cioè 0) su \"GO\".\n",
        "\n",
        "#### Falso - 8 Comici finiscono qui:\n",
        "- Rank <= 9.5 significa ogni comico con un rango pari o inferiore a 9,5 ​​seguirà la True freccia (a sinistra) e gli altri seguiranno la False freccia (a destra).\n",
        "\n",
        "- gini = 0.219 significa che circa il 22% dei campioni andrebbe in una delle 2 direzioni.\n",
        "\n",
        "- samples = 8 significa che sono rimasti 8 comici in questo ramo (8 comici con un grado superiore a 6,5).\n",
        "\n",
        "- value = [1, 7] significa che di questi 8 comici, 1 riceverà un \"NO\" e 7 un \"GO\".\n",
        "\n",
        "### Andiamo all'Ultimo step\n",
        "\n",
        "### Vero - 1 Comico finisce qui:\n",
        "- gini = 0.0 significa che tutti i campioni hanno ottenuto lo stesso risultato.\n",
        "\n",
        "- samples = 1 significa che è rimasto 1 comico in questo ramo (1 comico con 11,5 anni di esperienza o meno).\n",
        "\n",
        "- value = [1, 0] significa che 1 riceverà un \"NO\" e 0 un \"VAI\".\n",
        "\n",
        "### Falso - 3 Comici finiscono qui:\n",
        "- gini = 0.0 significa che tutti i campioni hanno ottenuto lo stesso risultato.\n",
        "\n",
        "- samples = 3 significa che in questo ramo sono rimasti 3 comici (3 comico con più di 11,5 anni di esperienza).\n",
        "\n",
        "- value = [0, 3] significa che 0 riceveranno un \"NO\" e 3 otterranno un \"GO\".\n",
        "\n",
        "## Prevediamo ora i nuovi valori\n",
        "Utilizziamo ora l'albero decisionale per prevedere nuovi valori,\n",
        "\n",
        "In questo esempio lo spettacolo ha protagonista un comico americano di 40 anni, con 10 anni di esperienza e un rank pari a 7.\n",
        "\n",
        "Utilizziamo quindi il metodo predict() per prevedere nuovi valori:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(dtree.predict([[40, 10, 7, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quale sarebbe la risposta se aumentassimo gli anni di esperienza?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(dtree.predict([[40, 12, 7, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Molto probabibilmente avremo in più esecuzioni Risultati diversi\n",
        "Vedrete che l'albero decisionale dà risultati diversi se lo eseguiamo abbastanza volte, anche alimentandolo con gli stessi dati.\n",
        "\n",
        "Questo perché l’albero decisionale non ci fornisce una risposta certa al 100%, si basa sulla probabilità di un risultato e la risposta varierà."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entriamo ancora più nel dettaglio degli alberi decisionali e foreste casuali"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhMRHnooEh6"
      },
      "source": [
        "Come abbiamo già detto, le foreste casuali sono un esempio di metodo d'insieme, nel senso che si basa sull'aggregazione dei risultati di un insieme di stimatori più semplici. Il risultato in qualche modo sorprendente con tali metodi d’insieme è che la somma può essere maggiore delle parti: cioè, una previsione a maggioranza tra un certo numero di stimatori può finire per essere migliore di quello di qualsiasi singolo stimatore! Vedremo alcuni esempi tra pochissimo, ora iniziamo con le importazioni standard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJFMJGpwoEh7",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L49nESIQoEh8"
      },
      "source": [
        "## Alberi decisionali"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUhzDgzKoEh8"
      },
      "source": [
        "Come detto, le foreste casuali sono basate sugli alberi decisionali, quindi è logico iniziare discutendo gli stessi alberi.\n",
        "\n",
        "Come sappiamo, gli alberi decisionali sono modi estremamente intuitivi per classificare o etichettare gli oggetti: basta porre una serie di domande progettate per concentrarsi sulla classificazione. A esempio, come visto se volessimo costruire un albero decisionale per classificare un animale che incontriamo durante un'escursione, potremmo costruire quello mostrato di seguito:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUBgUzjoEh8"
      },
      "source": [
        "![](dati/img/decision-tree.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EDA4i5foEh8"
      },
      "source": [
        "La suddivisione binaria lo rende estremamente efficiente: in un albero ben costruito, ogni domanda ridurrà il numero di opzioni di circa la metà, restringendo molto rapidamente le opzioni anche tra un gran numero di classi. Il trucco, ovviamente, sta nel decidere quali domande porre ad ogni passaggio. Nelle implementazioni di machine learning degli alberi decisionali, le domande generalmente assumono la forma di suddivisioni dei dati allineate agli assi: ovvero, ciascun nodo dell'albero divide i dati in due gruppi utilizzando un valore di interruzione all'interno di una delle funzionalità. Vediamo ora un esempio di questo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eokIVQaFoEh9"
      },
      "source": [
        "### Creare l'albero \n",
        "Consideriamo i seguenti dati bidimensionali, che hanno una delle quattro etichette di classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60aV4lfAoEh9",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "1538eb01-d35b-4ae0-ff2d-ddcf1a8d91aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvqUFRUGoEh9"
      },
      "source": [
        "Un semplice albero decisionale costruito su questi dati dividerà iterativamente i dati lungo l'uno o l'altro asse secondo un criterio quantitativo e ad ogni livello assegnerà l'etichetta della nuova regione secondo un voto di maggioranza dei punti al suo interno. Questa figura presenta una visualizzazione dei primi quattro livelli di un classificatore dell'albero decisionale per questi dati:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7aSS-VFoEh9"
      },
      "source": [
        "![](dati/img/decision-tree-4-levels.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpja8j5aoEh-"
      },
      "source": [
        "Si noti che dopo la prima divisione, ogni punto del ramo superiore rimane invariato, quindi non è necessario suddividere ulteriormente questo ramo. Ad eccezione dei nodi che contengono tutti lo stesso colore, ad ogni livello ogni regione è nuovamente divisa lungo una delle due caratteristiche.\n",
        "\n",
        "Questo processo di adattamento di un albero decisionale ai nostri dati, come già visto, può essere eseguito in Scikit-Learn con lo stimatore DecisionTreeClassifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-dYbgNmoEh-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier().fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8I5lOgWoEh-"
      },
      "source": [
        "Scriviamo ora una rapida funzione di utilità che ci aiuti a visualizzare l'output del classificatore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFMAqhg8oEh-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap, zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH2aZfUgoEh-"
      },
      "source": [
        "Ora possiamo esaminare come appare la classificazione dell'albero decisionale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8TfRicxoEh-",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6931c996-0372-4631-f69b-96189fb0860e"
      },
      "outputs": [],
      "source": [
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3EoqWQooEh-"
      },
      "source": [
        "### Decision Trees e Overfitting\n",
        "\n",
        "Come detto l'adattamento eccessivo risulta essere una proprietà generale degli alberi decisionali: è molto facile andare troppo in profondità nell'albero e quindi adattare i dettagli dei dati particolari piuttosto che le proprietà generali delle distribuzioni da cui sono tratti. \n",
        "Un modo per vedere questo adattamento eccessivo è guardare modelli addestrati su diversi sottoinsiemi di dati: \n",
        "ad esempio, in questa figura addestriamo due alberi diversi, ciascuno su metà dei dati originali:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOasEKrXoEh_"
      },
      "source": [
        "![](dati/img/decision-tree-overfitting.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdd2KDqoEh_"
      },
      "source": [
        "È chiaro che in alcuni punti i due alberi producono risultati coerenti (ad esempio nei quattro angoli), mentre in altri luoghi i due alberi danno classificazioni molto diverse (ad esempio nelle regioni tra due cluster qualsiasi). L'osservazione chiave è che le incoerenze tendono a verificarsi laddove la classificazione è meno certa e quindi, utilizzando le informazioni di entrambi questi alberi, potremmo ottenere un risultato migliore!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIGa1GJYoEh_"
      },
      "source": [
        "Proprio come l’utilizzo delle informazioni provenienti da due alberi migliora i nostri risultati, potremmo aspettarci che l’utilizzo delle informazioni provenienti da molti alberi migliorerebbe ulteriormente i nostri risultati."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KmgolH0oEh_"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Questa nozione, ovvero che più stimatori di overfitting possono essere combinati per ridurre l’effetto di questo overfitting, è ciò che sta alla base di un metodo di insieme che abbiamo già visto cioè il bagging . Come sappiamo il bagging fa uso di un insieme di stimatori paralleli, ognuno dei quali si adatta eccessivamente ai dati e calcola la media dei risultati per trovare una classificazione migliore. Un insieme di alberi decisionali randomizzati è noto come foresta casuale .\n",
        "\n",
        "Questo tipo di classificazione del bagging può essere eseguita manualmente utilizzando il meta-stimatore BaggingClassifier di Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aevv2um_oEh_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6f5212a0-0927-41d7-f718-de0a4284bba4"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(X, y)\n",
        "visualize_classifier(bag, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhrxqL0LoEh_"
      },
      "source": [
        "In questo esempio, abbiamo randomizzato i dati adattando a ciascun stimatore un sottoinsieme casuale pari all'80% dei punti di addestramento. In pratica, gli alberi decisionali vengono randomizzati in modo più efficace introducendo un po’ di stocasticità nel modo in cui vengono scelte le suddivisioni: in questo modo tutti i dati contribuiscono ogni volta all’adattamento, ma i risultati dell’adattamento mantengono ancora la casualità desiderata. Ad esempio, quando si determina su quale funzionalità suddividere, l'albero randomizzato potrebbe selezionare tra le principali funzionalità.\n",
        "\n",
        "In Scikit-Learn, un insieme ottimizzato di alberi decisionali randomizzati è implementato nello stimatore RandomForestClassifier, che si occupa automaticamente di tutta la randomizzazione. Tutto quello che dobbiamo fare è selezionare un numero di stimatori e molto rapidamente (in parallelo, se lo desideriamo) adatterà l'insieme di alberi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpCu4BroEh_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "56f954bf-f96b-496d-9cce-a414b8994d78"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "visualize_classifier(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMhC-kdKoEh_"
      },
      "source": [
        "Vediamo che calcolando la media di oltre 100 modelli trovati casualmente, ci ritroviamo con un modello complessivo che è molto più vicino alla nostra intuizione su come dovrebbe essere suddiviso lo spazio dei parametri."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxkc91PGoEiB"
      },
      "source": [
        "## Random Forest Regression\n",
        "\n",
        "Nella sezione precedente abbiamo considerato le foreste casuali nel contesto della classificazione, ma esse possono anche essere utilizzate nel caso di regressione (ovvero di variabili continue anziché categoriali). Lo stimatore da utilizzare per questo caso è RandomForestRegressor e la sintassi è molto simile a quanto visto in precedenza.\n",
        "\n",
        "Consideriamo i seguenti dati, ricavati dalla combinazione di un'oscillazione veloce e lenta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2QQS9DLoEiB",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "67e8034a-a634-4956-afac-8dd286d54a9f"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(42)\n",
        "x = 10 * rng.rand(200)\n",
        "\n",
        "def model(x, sigma=0.3):\n",
        "    fast_oscillation = np.sin(5 * x)\n",
        "    slow_oscillation = np.sin(0.5 * x)\n",
        "    noise = sigma * rng.randn(len(x))\n",
        "\n",
        "    return slow_oscillation + fast_oscillation + noise\n",
        "\n",
        "y = model(x)\n",
        "plt.errorbar(x, y, 0.3, fmt='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_PfJZxHoEiB"
      },
      "source": [
        "Utilizzando il regressore della foresta casuale, possiamo trovare la curva di adattamento migliore come segue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qazP7nozoEiB",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "3e0dd517-0ccd-4827-bf0a-288e2877a986"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest = RandomForestRegressor(200)\n",
        "forest.fit(x[:, None], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = forest.predict(xfit[:, None])\n",
        "ytrue = model(xfit, sigma=0)\n",
        "\n",
        "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
        "plt.plot(xfit, yfit, '-r')\n",
        "plt.plot(xfit, ytrue, '-k', alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC68ZxwxoEiB"
      },
      "source": [
        "Qui il modello reale è mostrato dalla curva grigia liscia, mentre il modello casuale della foresta è mostrato dalla curva rossa frastagliata. Come possiamo vedere, il modello di foresta casuale non parametrico è sufficientemente flessibile da adattarsi ai dati multiperiodo, senza che sia necessario specificare un modello multiperiodo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrqjOWAzoEiB"
      },
      "source": [
        "## Esempio: foresta casuale per classificare le cifre\n",
        "\n",
        "In precedenza abbiamo già analizzato il dataset delle cifre scritte a mano. Usiamolo di nuovo qui per vedere come è possibile utilizzare il classificatore di foresta casuale in questo contesto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLi3xufPoEiB",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "48880912-828b-4ea5-b88f-4cd801e89f57"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VpeTyF7oEiF"
      },
      "source": [
        "Per ricordarci cosa stiamo guardando, visualizzeremo i primi punti dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeBtwe60oEiF",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6fa862b3-35a1-4e6e-88de-57d3b06ac06a"
      },
      "outputs": [],
      "source": [
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the digits: each image is 8x8 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "\n",
        "    # label the image with the target value\n",
        "    ax.text(0, 7, str(digits.target[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ld1vKHoEiF"
      },
      "source": [
        "Possiamo classificare rapidamente le cifre utilizzando una foresta casuale come segue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQA56SG7oEiF",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
        "                                                random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt-zwTHAoEiF"
      },
      "source": [
        "Possiamo dare un'occhiata al report di classificazione per questo classificatore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCHF2SsboEiF",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "64925144-6d77-4a98-ff2d-bd12bf0868ef"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(ypred, ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvm6HSPQoEiG"
      },
      "source": [
        "E per buona misurazione, tracciamo la matrice di confusione:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W23KS5hToEiG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b1330cde-229c-4bcf-8cbc-6dfade5ceb8f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues')\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNLJiV3xoEiG"
      },
      "source": [
        "Ci rendiamo quindi conto che una foresta casuale semplice e non ottimizzata produce una classificazione molto accurata dei dati delle cifre."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
