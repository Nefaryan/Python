{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN per prevedere l'età delle lumache di mare\n",
    "Per capire nella pratica come funziona questo algoritmo useremo il set di dati Abalone, questo set contiene misurazioni dell'età di un gran numero di abaloni. Solo per informazione, ecco come appare un abalone:\n",
    "\n",
    "![abalone](dati/img/abalone.jpeg)\n",
    "\n",
    "Come possiamo vedere, gli abaloni sono piccole lumache di mare che assomigliano un po' alle cozze. \n",
    "\n",
    "## La dichiarazione sul problema dell'abalone\n",
    "\n",
    "L'età di un abalone può essere trovata tagliando il suo guscio e contando il numero di anelli sul guscio, in questo set di dati Abalone possiamo trovare le misurazioni dell'età di un gran numero di abalone insieme a molte altre misurazioni fisiche.\n",
    "\n",
    "L'obiettivo del progetto è sviluppare un modello in grado di prevedere l'età di un abalone basandosi esclusivamente su altre misurazioni fisiche, ciò consentirebbe ai ricercatori di stimare l'età dell'abalone senza dover tagliare il guscio e contare gli anelli.\n",
    "\n",
    "Applicheremo quindi un algoritmo kNN per trovare il punteggio di previsione più vicino possibile.\n",
    "\n",
    "## Importazione del set di dati Abalone\n",
    "\n",
    "Potremo scaricare manualmente il dataset e poi importarlo in Python utilizzando Pandas, ma è ancora più veloce lasciare fare tutto al codice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    ")\n",
    "abalone = pd.read_csv(url, header=None)\n",
    "\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che mancano ancora i nomi delle colonne, possiamo trovare questi nomi nel repository di machine learning dell'UCI https://archive.ics.uci.edu/dataset/1/abalone e possiamo aggiungerli al tuo DataFrame come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.columns = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole weight\",\n",
    "    \"Shucked weight\",\n",
    "    \"Viscera weight\",\n",
    "    \"Shell weight\",\n",
    "    \"Rings\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati importati ora dovrebbero essere più comprensibili, ma c'è un'altra cosa che dovremo fare: rimuovere la colonna Sex. L'obiettivo dell'esercizio attuale è utilizzare misurazioni fisiche per prevedere l'età dell'abalone, poiché il sesso non è una misura puramente fisica, lo rimuoveremo dal set di dati utilizzando .drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop(\"Sex\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiche descrittive dal set di dati Abalone\n",
    "Quando lavoriamo sull'apprendimento automatico, dobbiamo avere un'idea dei dati con cui stiamo lavorando.\n",
    "\n",
    "La variabile target di questo esercizio è Rings, quindi possiamo iniziare con quella, un istogramma ci fornirà una panoramica rapida e utile delle fasce di età che possiamo aspettarci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "abalone[\"Rings\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'istogramma mostra che la maggior parte degli abaloni nel set di dati ha tra i cinque e i quindici anelli, ma che è possibile arrivare fino a venticinque anelli, quindi capiamo subito che gli abaloni più vecchi sono sottorappresentati in questo set di dati, ciò sembra intuitivo, poiché le distribuzioni per età sono generalmente distorte in questo modo a causa dei processi naturali.\n",
    "\n",
    "Una seconda esplorazione rilevante è scoprire quali variabili, se presenti, hanno una forte correlazione con l’età, una forte correlazione tra una variabile indipendente e la variabile target sarebbe un buon segno, poiché confermerebbe che le misurazioni fisiche e l'età sono correlate.\n",
    "\n",
    "Come sappiamo, è possibile osservare la matrice di correlazione completa con correlation_matrix , le correlazioni più importanti sono quelle con la variabile target Rings. \n",
    "Possiamo ottenere quelle correlazioni in questo modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = abalone.corr()\n",
    "correlation_matrix[\"Rings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora guardando i coefficienti di correlazione Rings con le altre variabili sappiamo che più si avvicinano a 1, maggiore è la correlazione.\n",
    "\n",
    "Si può concludere che esiste almeno una certa correlazione tra le misurazioni fisiche degli abaloni adulti e la loro età, ma non è nemmeno molto alta, correlazioni molto elevate significano che possiamo aspettarci un processo di modellazione semplice, in questo caso invece dovremo provare a vedere quali risultati possiamo ottenere utilizzando l'algoritmo kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definire il \"più vicino\" utilizzando una definizione matematica di distanza\n",
    "\n",
    "Abbiamo detto che per trovare i punti dati più vicini al punto che dobbiamo prevedere, possiamo utilizzare una definizione matematica di distanza chiamata distanza euclidea. \n",
    "\n",
    "Senza tornare nello specifico sulla formula, per applicarla ai nostri dati, dobbiamo capire che i punti sono in realtà vettori, quindi è possibile calcolare la distanza tra loro calcolando la norma del vettore differenza.\n",
    "\n",
    "In Python possiamo farlo usando linalg.norm() da NumPy. \n",
    "Ecco un esempio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([2, 2])\n",
    "b = np.array([4, 4])\n",
    "np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo blocco di codice definiamo i punti dati come vettori, quindi calcoliamo la differenza tra due punti dati con norm(). In questo modo si ottiene direttamente la distanza tra due punti multidimensionali, anche se i punti sono multidimensionali, la distanza tra loro è ancora uno scalare o un singolo valore.\n",
    "\n",
    "## Trovare i k vicini\n",
    "\n",
    "Ora che conosciamo un modo per calcolare la distanza da qualsiasi punto a qualsiasi punto, possiamo usarlo per trovare i più vicini a un punto su cui vogliamo fare una previsione.\n",
    "\n",
    "Dobbiamo trovare un numero di vicini e quel numero è dato da k, il valore minimo di k è 1, ciò significa utilizzare un solo vicino per la previsione. Il massimo di vicini è invece il numero di punti dati di cui disponi, ciò significa utilizzare tutti i vicini. Come sappiamo quindi, il valore di k è qualcosa che l'utente definisce e in questo ci aiutano gli strumenti di ottimizzazione.\n",
    "\n",
    "Ora, per trovare i vicini più vicini in NumPy, torniamo al set di dati Abalone, come visto, dobbiamo definire le distanze sui vettori delle variabili indipendenti, quindi dobbiamo prima inserire il DataFrame pandas in un array NumPy usando l'attributo .values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone.drop(\"Rings\", axis=1)\n",
    "X = X.values\n",
    "y = abalone[\"Rings\"]\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo provare ad applicare l'algoritmo kNN con k= 3 su un nuovo abalone che abbia le seguenti misure fisiche:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_point = np.array([\n",
    "    0.569552,\n",
    "    0.446407,\n",
    "    0.154437,\n",
    "    1.016849,\n",
    "    0.439051,\n",
    "    0.222526,\n",
    "    0.291208,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passaggio successivo consiste nel calcolare le distanze tra questo nuovo punto e ciascuno dei punti nel set di dati Abalone utilizzando il seguente codice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.linalg.norm(X - new_data_point, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora abbiamo un vettore di distanze e dobbiamo scoprire quali sono i tre punti più vicini, per fare ciò è necessario trovare gli ID delle distanze minime. Possiamo utilizzare un metodo chiamato .argsort() per ordinare l'array dal più basso al più alto e possiamo prendere i primi k elementi per ottenere gli indici dei k più vicini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nearest_neighbor_ids = distances.argsort()[:k]\n",
    "nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voto o media di più vicini\n",
    "\n",
    "Dopo aver identificato gli indici dei tre più vicini dal nostro nuovo abalone di età sconosciuta, dobbiamo combinare questi vicini in una previsione.\n",
    "\n",
    "Come primo passo, dobbiamo trovare le età per questi tre vicini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbor_rings = y[nearest_neighbor_ids]\n",
    "nearest_neighbor_rings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media per la regressione\n",
    "Come sappiamo nei problemi di regressione, la variabile target è numerica, combiniamo quindi i più vicini in un'unica previsione prendendo la media dei loro valori della variabile target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nearest_neighbor_rings.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otterremo un valore di 10 for prediction. Ciò significa che la previsione dei 3 vicini più vicini per il nuovo Abalone è 10.\n",
    "\n",
    "## Modalità di classificazione\n",
    "Nei problemi di classificazione invece la variabile obiettivo è categorica, quindi non usiamo la media ma la moda, che come già sappiamo, è il valore che ricorre più spesso, ciò significa che contiamo le classi di tutti i vicini e mantieniamo la classe più comune, la previsione è quindi il valore che ricorre più spesso tra i vicini.\n",
    "\n",
    "Se esistono più mode, ci sono più soluzioni possibili, potremmo selezionare un vincitore finale in modo casuale tra i vincitori, potremmo anche prendere la decisione finale in base alle distanze dei vicini, nel qual caso verrebbe mantenuta la moda dei più vicini.\n",
    "\n",
    "Poiché l'esempio dell'abalone non è un caso di classificazione, partiremo da altri dati di esempio.\n",
    "\n",
    "Come abbiamo già visto, è possibile quindi calcolare la moda utilizzando la funzione SciPy mode(), ma dalla versione 1.11.0 è stata tolta la possibilità di calcolare la moda su dati non numerici, creremo quindi una funzione personalizzata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_neighbors = np.array([\"A\", \"B\", \"B\", \"C\"])\n",
    "\n",
    "\n",
    "def mode(lst):\n",
    "     \n",
    "    # creating a dictionary\n",
    "    freq = {}\n",
    "    for i in lst:\n",
    "       \n",
    "        # mapping each value of list to a \n",
    "        # dictionary\n",
    "        freq.setdefault(i, 0)\n",
    "        freq[i] += 1\n",
    "         \n",
    "    # finding maximum value of dictionary\n",
    "    hf = max(freq.values())\n",
    "     \n",
    "    # creating an empty list\n",
    "    hflst = []\n",
    "     \n",
    "    # using for loop we are checking for most \n",
    "    # repeated value\n",
    "    for i, j in freq.items():\n",
    "        if j == hf:\n",
    "            hflst.append(i)\n",
    "             \n",
    "    # returning the result\n",
    "    return hflst\n",
    "\n",
    "# calling mode() function and passing list\n",
    "# as argument\n",
    "print(mode(class_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementiamo l'algoritmo kNN in Python utilizzando scikit-learn\n",
    "Sebbene codificare un algoritmo da zero è ottimo per scopi di apprendimento, di solito non è molto pratico quando si lavora su un'attività di machine learning, vedremo ora quindi l'implementazione dell'algoritmo kNN utilizzato scikit-learn.\n",
    "\n",
    "### Suddivisione dei dati in set di training e test per la valutazione del modello\n",
    "In questa sezione valuteremo la qualità del modello kNN di abalone, nelle sezioni precedenti avevamo un focus tecnico, ma ora avremo un punto di vista più pragmatico e orientato ai risultati.\n",
    "\n",
    "Esistono diversi modi per valutare i modelli, ma come abbiamo già visto il più comune e semplice è la suddivisione in train-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adattamento di una regressione kNN in scikit-learn al set di dati Abalone\n",
    "Come sappiamo, per adattare il modello da scikit-learn iniziamo creando un modello della classe corretta dopo scegliamo i valori per i nostri iperparametri.\n",
    "Per l'algoritmo kNN, dovremo scegliere il valore per k, che viene chiamato n_neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver scelto i tre più vicini, facciamo allenare il modello con .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizziamo scikit-learn per ispezionare l'adattamento del modello\n",
    "Esaminiamo ora alcune funzioni che è possibile utilizzare per valutare l'adattamento.\n",
    "\n",
    "Come sappiamo sono disponibili molti parametri di valutazione per la regressione, ma utilizzeremo ora uno dei più comuni, l' errore quadratico medio (RMSE) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "train_preds = knn_model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, train_preds)\n",
    "rmse = sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo codice, abbiamo calcolato l'RMSE sui dati di addestramento, ma per un risultato più realistico, dovremo valutare le prestazioni sui dati non inclusi nel modello. Questo è il motivo per cui per cui abbiamo mantenuto il set di test separato. Andiamo quindi a utilizzare la stessa funzione di prima sui dati di test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = knn_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, test_preds)\n",
    "rmse = sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo RMSE più realistico è leggermente più alto di prima, possiamo interpretarlo come se avesse, in media, un errore di 1.65 anni con i dati di train e 2.37 anni con i dati di test.\n",
    "\n",
    "Fino ad ora, abbiamo utilizzato solo l'algoritmo kNN scikit-learn pronto all'uso, non abbiamo ancora effettuato alcuna regolazione degli iperparametri e una scelta casuale per k. \n",
    "È possibile osservare una differenza relativamente ampia tra l'RMSE sui dati di addestramento e l'RMSE sui dati di test, ciò significa che il modello soffre di un adattamento eccessivo ai dati di addestramento: non si generalizza bene.\n",
    "\n",
    "### Tracciare l'adattamento del tuo modello\n",
    "Un'ultima cosa da considerare prima di iniziare a migliorare il modello è l'effettiva vestibilità del tuo modello, per capire cosa ha imparato il modello, possiamo visualizzare come sono state effettuate le previsioni utilizzando Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(\n",
    "    X_test[:, 0], X_test[:, 1], c=test_preds, s=50, cmap=cmap\n",
    ")\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo blocco di codice, abbiamo utilizzato Seaborn per creare un grafico a dispersione della prima e della seconda colonna di X_test con le prime due colonne di array X_test[:,0]e X_test[:,1]. Ricordiamo che le prime due colonne sono Length e Diameter che Sono fortemente correlate, come abbiamo visto nella tabella delle correlazioni.\n",
    "\n",
    "Utilizziamo poi c per specificare che i valori previsti ( test_preds) devono essere utilizzati come barra dei colori. L'argomento s invece viene utilizzato per specificare la dimensione dei punti nel grafico a dispersione. Infine cmapper si utilizza per specificare la mappa dei colori.\n",
    "\n",
    "#### Nel grafico che otteniamo\n",
    "ogni punto è un abalone del set di prova, con la sua lunghezza effettiva e il suo diametro effettivo rispettivamente sugli assi X e Y. Il colore del punto riflette l'età prevista, possiamo vedere che più un abalone è lungo e grande, maggiore è la sua età prevista. Questo è logico ed è un segnale positivo, significa che il modello sta imparando qualcosa che sembra corretto.\n",
    "\n",
    "Per confermare se questa tendenza esiste nei dati effettivi dell'abalone, possiamo fare lo stesso per i valori effettivi semplicemente sostituendo la variabile utilizzata per c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(\n",
    "    X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap=cmap\n",
    ")\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il grafico conferma che la tendenza che il modello sta apprendendo ha davvero senso.\n",
    "\n",
    "### Migliorare le prestazioni di kNN nell'uso di scikit-learnGridSearchCV\n",
    "\n",
    "In questo esempio abbiamo sempre lavorato con k=3, ma come sappiamo il valore migliore k è qualcosa che dobbiamo trovare empiricamente per ciascun set di dati.\n",
    "\n",
    "Per trovare il miglior valore per k, utilizzeremo come già visto in precedenza lo strumento chiamato GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"n_neighbors\": range(1, 50)}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo già visto, GridSearchCV adatta ripetutamente i regressori kNN su una parte dei dati e testa le prestazioni sulla restante parte dei dati, facendo ciò  si otterrà una stima affidabile delle prestazioni predittive di ciascuno dei valori per k (In questo esempio vengono testati i valori da 1 a 50).\n",
    "\n",
    "Alla fine potremo accedere al valore più perfomanete con .best_params_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo quindi vedere che la scelta 25 come valore di k produrrà le migliori prestazioni predittive. Ora che sappiamo quale k è il miglior valore, possiamo vedere come influisce sulle prestazioni dei dati di train e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_grid = gridsearch.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, train_preds_grid)\n",
    "train_rmse = sqrt(train_mse)\n",
    "test_preds_grid = gridsearch.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "\n",
    "print(train_rmse)\n",
    "\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che l'errore di addestramento è peggiore di prima, ma l'errore di test è migliore di prima, ciò significa che grazie al nostro lavoro il modello si adatta meno fedelmente ai dati di addestramento, quindi l'utilizzo GridSearchCV per trovare un valore k ha ridotto il problema dell'adattamento eccessivo ai dati di training.\n",
    "\n",
    "### Aggiunta della media ponderata dei vicini in base alla distanza\n",
    "\n",
    "Vedremo ora come migliorare ancora di più le prestazioni, testeremo quindi se le prestazioni del modello saranno migliori quando eseguiamo la previsione utilizzando una media ponderata anziché una media regolare, ciò significa che i vicini più lontani influenzeranno meno fortemente la previsione.\n",
    "\n",
    "Possiamo farlo impostando l'iperparametro weights sul valore di \"distance\", questo cambiamento potrebbe avere un impatto sul valore ottimale di k, pertanto, riutilizzeremo nuovamente GridSearchCV per capire quale tipo di media dovremmo utilizzare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_neighbors\": range(1, 50),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "gridsearch.best_params_\n",
    "\n",
    "test_preds_grid = gridsearch.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'applicazione di una media ponderata anziché di una media regolare ha ridotto l'errore di previsione da 2.17 a 2.1634, anche se questo non è un enorme miglioramento, è comunque un miglioramento e sopratutto potrebbe essere invece un miglioramento molto più evidente su altri set di dati.\n",
    "\n",
    "### Ulteriore miglioramento di kNN in scikit-learn con Bagging\n",
    "\n",
    "Come terzo passaggio per l'ottimizzazione dell'algoritmo kNN, è possibile utilizzare il bagging un metodo d'insieme, ovvero un metodo che prende un modello di apprendimento automatico relativamente semplice e si adatta a un gran numero di tali modelli con lievi variazioni in ogni adattamento. Il bagging utilizza spesso alberi decisionali (che vedremo più avanti), ma anche con l'algoritmo kNN funziona perfettamente.\n",
    "\n",
    "I metodi insieme sono spesso più performanti dei modelli singoli, un modello può sbagliarsi di tanto in tanto, ma la media di un centinaio di modelli dovrebbe sbagliarsi meno spesso. È probabile che gli errori dei diversi modelli individuali si medino a vicenda e la previsione risultante sarà meno variabile.\n",
    "\n",
    "Possiamo utilizzare scikit-learn per applicare il bagging alla regressione kNN utilizzando i seguenti passaggi:\n",
    "\n",
    "1. Per prima cosa, creiamo il file KNeighborsRegressor con le migliori scelte per k e weights che abbiamo ottenuto da GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = gridsearch.best_params_[\"n_neighbors\"]\n",
    "best_weights = gridsearch.best_params_[\"weights\"]\n",
    "bagged_knn = KNeighborsRegressor(\n",
    "    n_neighbors=best_k, weights=best_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Quindi importiamo la classe BaggingRegressor da scikit-learn e creiamo una nuova istanza con 100 stimatori utilizzando il modello bagged_knn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_model = BaggingRegressor(bagged_knn, n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ora fittiamo i dati e poi possiamo fare una previsione e calcolare l'RMSE per vedere se è migliorato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model.fit(X_train, y_train)\n",
    "test_preds_grid = bagging_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'errore di previsione sul kNN è 2.1616, che è leggermente inferiore all'errore precedente ottenuto. \n",
    "### Confronto dei quattro modelli\n",
    "In tre passaggi incrementali abbiamo migliorato ulteriromente le prestazioni predittive dell'algoritmo. \n",
    "\n",
    "Di seguito la tabella che mostra un riepilogo dei diversi modelli e delle loro prestazioni:\n",
    "\n",
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Modello</font></font></th>\n",
    "<th><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Errore</font></font></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\"><code>k</code> Arbitrario</font></font></td>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">2.37</font></font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>GridSearchCV</code> <font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\"> per</font></font> <code>k</code></td>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">2.17</font></font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>GridSearchCV</code> <font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\"> per </font></font> <code>k</code> <font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\"> e </font></font> <code>weights</code></td>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">2.1634</font></font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">Bagging e </font></font> <code>GridSearchCV</code></td>\n",
    "<td><font style=\"vertical-align: inherit;\"><font style=\"vertical-align: inherit;\">2.1616</font></font></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Vediamo i quattro modelli dal più semplice al più complesso. L'ordine di complessità corrisponde all'ordine delle metriche di errore. Il modello con un random k ha ottenuto i risultati peggiori, mentre il modello con il bagging e GridSearchCV i risultati migliori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 1\n",
    "Utilizzare l'algoritmo k-Nearest Neighbors (k-NN) per la classificazione utilizzando il dataset \"Iris\" disponibile nella libreria scikit-learn:\n",
    "\n",
    "### Passaggi da completare:\n",
    "\n",
    "- Caricamento del dataset:\n",
    "\n",
    "1. Carica il dataset \"Iris\" dalla libreria scikit-learn.\n",
    "2. Esplora il dataset per comprendere le caratteristiche presenti, i loro tipi e la distribuzione delle classi di output.\n",
    "\n",
    "- Preprocessing dei dati:\n",
    "\n",
    "1. Dividi il dataset in features (variabili indipendenti) e target (variabile dipendente).\n",
    "2. Dividi il dataset in training set e test set utilizzando una proporzione del 80-20.\n",
    "\n",
    "- Creazione del modello k-NN:\n",
    "\n",
    "1. Crea un modello k-NN utilizzando il numero di vicini desiderato.\n",
    "\n",
    "- Addestramento del modello:\n",
    "\n",
    "1. Addestra il modello k-NN sul training set.\n",
    "\n",
    "- Valutazione del modello:\n",
    "\n",
    "1. Valuta le prestazioni del modello utilizzando il test set.\n",
    "2. Calcola l'accuratezza del modello.\n",
    "3. Visualizza il report di classificazione che include precision, recall e F1-score per ogni classe.\n",
    "4. Visualizza la matrice di confusione per valutare le prestazioni del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 2\n",
    "In questo esercizio, userete il dataset \"Breast Cancer Wisconsin (Diagnostic)\" disponibile nella libreria scikit-learn.\n",
    "\n",
    "### Passaggi da completare:\n",
    "\n",
    "- Caricamento del dataset:\n",
    "\n",
    "1. Carica il dataset \"Breast Cancer Wisconsin (Diagnostic)\" dalla libreria scikit-learn.\n",
    "2. Esplora il dataset per comprendere le caratteristiche presenti, i loro tipi e la distribuzione delle classi di output.\n",
    "\n",
    "- Preprocessing dei dati:\n",
    "\n",
    "1. Dividi il dataset in features (variabili indipendenti) e target (variabile dipendente).\n",
    "2. Dividi il dataset in training set e test set utilizzando una proporzione del 70-30.\n",
    "\n",
    "- Standardizzazione dei dati:\n",
    "\n",
    "1. Standardizza le features utilizzando lo StandardScaler di scikit-learn.\n",
    "\n",
    "- Creazione del modello k-NN:\n",
    "\n",
    "1. Crea un modello k-NN specificando il numero di vicini desiderato.\n",
    "\n",
    "- Addestramento del modello:\n",
    "\n",
    "1. Addestra il modello k-NN sul training set.\n",
    "\n",
    "- Valutazione del modello:\n",
    "\n",
    "1. Valuta le prestazioni del modello utilizzando il test set.\n",
    "2. Calcola l'accuratezza del modello.\n",
    "3. Visualizza il report di classificazione che include precision, recall e F1-score per ogni classe.\n",
    "4. Visualizza la matrice di confusione per valutare le prestazioni del modello."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
