{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressione logistica in Python con scikit-learn: Esempio 4 riconoscimento della scrittura\n",
    "Questo esempio riguarda il riconoscimento delle immagini, per essere pi√π precisi, lavoreremo sul riconoscimento delle cifre scritte a mano. Utilizzeremo un set di dati con 1797 osservazioni, ognuna delle quali √® un'immagine di una cifra scritta a mano, ogni immagine ha 64 px, con una larghezza di 8 px e un'altezza di 8 px.\n",
    "Gli input (ùê±) sono vettori con 64 dimensioni o valori, ciascun vettore di input descrive un'immagine, ciascuno dei 64 valori rappresenta un pixel dell'immagine. \n",
    "- I valori di input sono numeri interi compresi tra 0 e 16, a seconda della tonalit√† di grigio del pixel corrispondente. \n",
    "- L'output (ùë¶) per ciascuna osservazione √® un numero intero compreso tra 0 e 9, coerente con la cifra sull'immagine. \n",
    "Ci sono dieci classi in totale, ciascuna corrispondente a un'immagine.\n",
    "\n",
    "### Passaggio 1: importiamo i pacchetti\n",
    "Dovremo importare Matplotlib, NumPy e diverse funzioni e classi da scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 2 a: ottieniamo i dati\n",
    "Possiamo prendere il set di dati direttamente da scikit-learn con load_digits(). Restituisce una tupla di input e output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questi sono i dati con cui lavorare, X √® un array multidimensionale con 1797 righe e 64 colonne che contiene numeri interi da 0 a 16. \n",
    "y √® un array unidimensionale con 1797 numeri interi compresi tra 0 e 9.\n",
    "\n",
    "Proviamo a visualizzare qualche cifra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(30, 30))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Training: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 2 b: suddivisione dei dati\n",
    "Come abbiamo visto, √® una pratica buona e ampiamente adottata dividere il set di dati con cui stiamo lavorando in due sottoinsiemi:\n",
    "- set di addestramento \n",
    "- set di test. \n",
    "Utilizzeremo quindi il set di training per allenare il nostro modello e valuteremo le prestazioni con il set di prova. Come sappiamo questo approccio consente una valutazione imparziale del modello.\n",
    "\n",
    "Un modo per suddividere il set di dati in set di training e test √® applicare train_test_split() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split() richiede opzionalmente test_size, con che determina la dimensione del set di test e random_state che definisce lo stato del generatore di numeri pseudo-casuali, oltre ad altri argomenti opzionali. \n",
    "\n",
    "Una volta suddivisi i dati, possiamo dimenticarci momentaneamente di x_test e y_test finch√© non completiamo il modello.\n",
    "\n",
    "### Passaggio 2 c: ridimensionare i dati\n",
    "Come gi√† visto, la standardizzazione √® il processo di trasformazione dei dati in modo tale che la media di ciascuna colonna diventi uguale a zero e la deviazione standard di ciascuna colonna sia pari a uno. In questo modo si ottiene la stessa scala per tutte le colonne. Procediamo nel seguente modo per standardizzare i dati:\n",
    "\n",
    "- Calcoliamo la media e la deviazione standard per ciascuna colonna.\n",
    "- Sottraiamo la media corrispondente da ciascun elemento.\n",
    "- Dividiamo la differenza ottenuta per la deviazione standard corrispondente.\n",
    "\n",
    "Sebbene, come abbiamo visto in precedenza, in molti casi non sia necessario standardizzare i dati di input in questo caso √® consigliabile per utilizzarli per la regressione logistica, infatti la standardizzazione potrebbe migliorare le prestazioni del algoritmo.\n",
    "\n",
    "Possiamo standardizzare i nostri input creando un'istanza di StandardScaler e chiamando .fit_transform():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".fit_transform() come sappiamo adatta l'istanza di StandardScaler all'array passato come argomento, trasformando questo array restituisce il nuovo array standardizzato. Ora, x_train √® un array di input standardizzato.\n",
    "\n",
    "### Passaggio 3: creiamo un modello e addestriamolo\n",
    "Ancora una volta, creiamo un'istanza di LogisticRegression e richiamiamo .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', C=0.05, multi_class='ovr',\n",
    "                           random_state=0)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando lavoriamo con problemi con pi√π di due classi, √® possibile specificare il multi_class parametro di LogisticRegression, esso determina come risolvere il problema:\n",
    "\n",
    "- 'ovr' dice di adattare il binario a ogni classe.\n",
    "- 'multinomial' dice di applicare l'adattamento multinomiale delle perdite.\n",
    "\n",
    "### Passaggio 4: valutare il modello\n",
    "Dovremo valutare il modello in modo simile a quello che abbiamo gi√† fatto negli esempi precedenti, con la differenza che utilizzeremo principalmente x_test e y_test, che sono i sottoinsiemi non applicati per l'addestramento. Se abbiamo deciso di standardizzare x_train, il modello ottenuto si basa sui dati ridimensionati, quindi x_test dovrebbe essere ridimensionato anche con la stessa istanza di StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√à cos√¨ che ottieniamo un nuovo file x_test, in questo caso usiamo .transform() che trasforma solo l'argomento senza adattare lo scaler.\n",
    "\n",
    "A questo punto √® possibile ottenere le previsioni con .predict():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variabile y_pred √® ora associata a una matrice degli output previsti, ora otteniamo la precisione con .score() con i dati di training e quelli di test:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(x_train, y_train))\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo ottenere la matrice di confusione e vederla graficamente considerando che conterr√† 100 numeri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('Predicted outputs', fontsize=15, color='black')\n",
    "ax.set_ylabel('Actual outputs', fontsize=15, color='black')\n",
    "ax.xaxis.set(ticks=range(10))\n",
    "ax.yaxis.set(ticks=range(10))\n",
    "ax.set_ylim(9.5, -0.5)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa √® la mappa termica che illustra la matrice di confusione con numeri e colori. Possiamo vedere che le sfumature del viola rappresentano numeri piccoli (come 0, 1 o 2), mentre il verde e il giallo mostrano numeri molto pi√π grandi (27 e oltre).\n",
    "\n",
    "I numeri sulla diagonale principale (27, 32, ‚Ä¶, 36) mostrano il numero di previsioni corrette dal set di test. Ad esempio, ci sono 27 immagini con zero, 32 immagini con uno e cos√¨ via che sono classificate correttamente. Altri numeri corrispondono a previsioni errate. A esempio, il numero 1 nella terza riga e nella prima colonna mostra che esiste un'immagine con il numero 2 classificato erroneamente come 0.\n",
    "\n",
    "Infine, possiamo ottenere il report sulla classificazione come stringa o dizionario con classification_report():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 2\n",
    "Partendo dal dataset al seguente link https://www.kaggle.com/datasets/dragonheir/logistic-regression/data create il modello pi√π preciso possibile per prevedere gli acquisti in base alla caratteristica del cliente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
