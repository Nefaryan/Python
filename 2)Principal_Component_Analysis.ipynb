{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJzYH82VNTi7"
      },
      "source": [
        "#  Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "by5uzzrcNTi9"
      },
      "source": [
        "Andiamo ora ad approfondire quello che è forse uno degli algoritmi non supervisionati più utilizzati, l'analisi delle componenti principali (PCA). \n",
        "PCA come abbiamo già visto è fondamentalmente un algoritmo di riduzione della dimensionalità, ma può anche essere utile come strumento per la visualizzazione, per il filtraggio del rumore, per l'estrazione e l'ingegneria delle funzionalità e molto altro ancora. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-J7r04xHNTi-"
      },
      "source": [
        "## Entriamo nel dettaglio\n",
        "\n",
        "L'analisi delle componenti principali come sappiamo è un metodo veloce e flessibile non supervisionato per la riduzione della dimensionalità nei dati. Il suo comportamento è più semplice da visualizzare osservando un set di dati bidimensionale, consideriamo quindi i seguenti 200 punti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uBbRKjmPNTi-",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "9521b94d-2ca1-44aa-988e-50094babfa58"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create point\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rZxyQs-PNTi-"
      },
      "source": [
        "A prima vista, è chiaro che esiste una relazione quasi lineare tra le variabili x e y, ciò ricorda i dati di regressione lineare che abbiamo esplorato in precedenza, ma la impostazione del problema qui è leggermente diversa: invece di tentare di prevedere i valori y dai valori x, come sappiamo il problema di apprendimento non supervisionato tenta di apprendere la relazione tra i valori x e y.\n",
        "\n",
        "Nell'analisi delle componenti principali, questa relazione viene quantificata trovando un elenco degli assi principali nei dati e utilizzando tali assi per descrivere il set di dati. Utilizzando lo stimatore PCA di Scikit-Learn, possiamo calcolarlo come segue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IZp6CXndNTi_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b1897df7-c22c-40c4-cab1-84864e7bf184"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "74cVHoPGNTi_"
      },
      "source": [
        "L'adattamento apprende alcune quantità dai dati, soprattutto i \"componenti\" e la \"varianza spiegata\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YRrcP10xNTi_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "0e95f240-0352-4b5e-b3b6-949e2edbc0fe"
      },
      "outputs": [],
      "source": [
        "print(pca.components_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "19W4OK_zNTi_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "de8e65b3-2900-4918-fa7f-8654473dfe5a"
      },
      "outputs": [],
      "source": [
        "print(pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GkH1eBvmNTi_"
      },
      "source": [
        "Per vedere cosa significano questi numeri, visualizziamoli come vettori sui dati di input, utilizzando i \"componenti\" per definire la direzione del vettore e la \"varianza spiegata\" per definire la lunghezza quadrata del vettore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3OEW2ArGNTi_",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "55f0e1bf-f116-463d-daf3-1dd334150c75"
      },
      "outputs": [],
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0)\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "OPRiNTfXNTjA"
      },
      "source": [
        "Questi vettori rappresentano gli assi principali dei dati e la lunghezza del vettore è un'indicazione di quanto \"importante\" sia quell'asse nel descrivere la distribuzione dei dati; più precisamente, è una misura della varianza dei dati quando proiettati su quell'asse. La proiezione di ciascun punto dati sugli assi principali sono le \"componenti principali\" dei dati.\n",
        "\n",
        "Se tracciamo queste componenti principali accanto ai dati originali, vediamo i grafici mostrati qui:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XqgLZUfeNTjA"
      },
      "source": [
        "![](dati/img/05.09-PCA-rotation.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wbBkIiHsNTjA"
      },
      "source": [
        "Questa trasformazione dagli assi dei dati agli assi principali è una trasformazione affine , il che significa sostanzialmente che è composta da traslazione, rotazione e ridimensionamento uniforme.\n",
        "\n",
        "Sebbene questo algoritmo per trovare i componenti principali possa sembrare solo una curiosità matematica, risulta avere applicazioni di vasta portata nel mondo dell'apprendimento automatico e dell'esplorazione dei dati."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "U3xiWdAlNTjA"
      },
      "source": [
        "### PCA e riduzione della dimensionalità\n",
        "\n",
        "L'utilizzo della PCA per la riduzione della dimensionalità comporta l'azzeramento di uno o più dei componenti principali più piccoli, ottenendo una proiezione dei dati a dimensione inferiore che preserva la massima varianza dei dati.\n",
        "\n",
        "Ecco un esempio di utilizzo della PCA come trasformazione di riduzione della dimensionalità:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Im05SmhvNTjA",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "cf890e6f-308e-4f35-b82d-8196b22d59d5"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_7GXYmXUNTjA"
      },
      "source": [
        "I dati trasformati sono stati ridotti a un'unica dimensione. Per comprendere l'effetto di questa riduzione della dimensionalità, possiamo eseguire la trasformazione inversa di questi dati ridotti e tracciarli insieme ai dati originali:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MemkC24xNTjA",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "587b9899-aeec-45de-9cc7-5c51f10b31b8"
      },
      "outputs": [],
      "source": [
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
        "plt.axis('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QIcuQKSCNTjA"
      },
      "source": [
        "I punti azzurri sono i dati originali, mentre i punti arancioni sono la versione proiettata. Ciò rende chiaro cosa significa una riduzione della dimensionalità della PCA: le informazioni lungo l'asse o gli assi principali meno importanti vengono rimosse, lasciando solo i componenti dei dati con la varianza più elevata. La frazione di varianza che viene eliminata (proporzionale alla diffusione dei punti attorno alla linea formata in questa figura) è approssimativamente una misura di quanta \"informazione\" viene scartata in questa riduzione della dimensionalità.\n",
        "\n",
        "Questo set di dati di dimensione ridotta è in un certo senso \"abbastanza buono\" per codificare le relazioni più importanti tra i punti: nonostante la riduzione della dimensione dei dati del 50%, la relazione complessiva tra i punti dati viene per lo più preservata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "O8ErtFnzNTjA"
      },
      "source": [
        "### PCA per la visualizzazione: cifre scritte a mano\n",
        "\n",
        "L'utilità della riduzione della dimensionalità potrebbe non essere del tutto evidente solo in due dimensioni, ma diventa molto più chiara quando si esaminano dati ad alta dimensionalità. Per capirlo, diamo una rapida occhiata pratica all'applicazione della PCA ai dati in cifre scritte a mano di cui abbiamo già parlato .\n",
        "\n",
        "Iniziamo caricando i dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3aLNmeP2NTjA",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "761a6397-ad21-429e-b3d2-d6d7953e463b"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Qk32XD9_NTjA"
      },
      "source": [
        "Ricordiamo che i dati sono costituiti da immagini di 8×8 pixel, il che significa che sono a 64 dimensioni. Per ottenere un'intuizione sulle relazioni tra questi punti, possiamo utilizzare PCA per proiettarli su un numero di dimensioni più gestibile, diciamo due:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Sc_fYAdCNTjA",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5015f886-e658-4a3c-821b-a3aad696c028"
      },
      "outputs": [],
      "source": [
        "pca = PCA(2)  # project from 64 to 2 dimensions\n",
        "projected = pca.fit_transform(digits.data)\n",
        "print(digits.data.shape)\n",
        "print(projected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "HP-HIxXQNTjA"
      },
      "source": [
        "Ora possiamo tracciare le prime due componenti principali di ciascun punto per conoscere i dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Xv8XgfLXNTjA",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "7a78ba3c-4fb0-4a91-dd5d-d1acee4d6ffc"
      },
      "outputs": [],
      "source": [
        "plt.scatter(projected[:, 0], projected[:, 1],\n",
        "            c=digits.target, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FSwvLpj7NTjB"
      },
      "source": [
        "Ricordiamo cosa significano questi componenti: i dati completi sono una nuvola di punti a 64 dimensioni e questi punti sono la proiezione di ciascun punto dati lungo le direzioni con la varianza maggiore. In sostanza, abbiamo trovato l'allungamento e la rotazione ottimali nello spazio a 64 dimensioni che ci consentono di vedere la disposizione delle cifre in due dimensioni, e lo abbiamo fatto in modo non supervisionato, cioè senza riferimento alle etichette."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "lTfR31wDNTjB"
      },
      "source": [
        "### Cosa significano i componenti? \n",
        "\n",
        "Possiamo andare un po' oltre e cominciare a chiederci cosa significano le dimensioni ridotte . Questo significato può essere compreso in termini di combinazioni di vettori di base. A esempio, ogni immagine nel set di training è definita da una raccolta di valori di 64 pixel, che chiameremo vettore X:\n",
        "\n",
        "$$\n",
        "x = [x_1, x_2, x_3 \\cdots x_{64}]\n",
        "$$\n",
        "\n",
        "Un modo in cui possiamo gestirli è in termini di pixel, cioè, per costruire l'immagine, moltiplichiamo ciascun elemento del vettore per il pixel che descrive, quindi sommiamo i risultati per costruire l'immagine:\n",
        "\n",
        "$$\n",
        "{\\rm image}(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots x_{64} \\cdot{\\rm (pixel~64)}\n",
        "$$\n",
        "\n",
        "Un modo in cui potremmo immaginare di ridurre la dimensione di questi dati è quello di azzerare tutti questi vettori di base tranne alcuni. A esempio, se utilizziamo solo i primi otto pixel, otteniamo una proiezione dei dati in otto dimensioni, ma non riflette molto l'intera immagine: abbiamo eliminato quasi il 90% dei pixel!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZMs41M3MNTjB"
      },
      "source": [
        "![](dati/img/digits-pixel-components.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Ba6LCeYrNTjB"
      },
      "source": [
        "La riga superiore di pannelli mostra i singoli pixel, mentre la riga inferiore mostra il contributo cumulativo di questi pixel alla costruzione dell'immagine. Utilizzando solo otto componenti basati sui pixel, possiamo costruire solo una piccola porzione dell'immagine da 64 pixel. Se continuassimo questa sequenza e utilizzassimo tutti i 64 pixel, recupereremo l'immagine originale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "D24evjSDNTjB"
      },
      "source": [
        "Ma la rappresentazione in pixel non è l’unica scelta di base, possiamo infatti anche utilizzare altre funzioni di base, che contengono ciascuna un contributo predefinito da ciascun pixel, e scrivere qualcosa di simile\n",
        "\n",
        "$$\n",
        "image(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
        "$$\n",
        "\n",
        "La PCA può essere pensata come un processo di scelta delle funzioni di base ottimali, in modo tale che sommare solo le prime di esse è sufficiente per ricostruire adeguatamente la maggior parte degli elementi nel set di dati. Le componenti principali, che fungono da rappresentazione a bassa dimensionalità dei nostri dati, sono semplicemente i coefficienti che moltiplicano ciascuno degli elementi di questa serie. Questa figura mostra una rappresentazione simile della ricostruzione di questa cifra utilizzando la media più le prime otto funzioni base PCA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "t7WvTH7jNTjC"
      },
      "source": [
        "![](dati/img/digits-pca-components.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8Bk0nwvWNTjC"
      },
      "source": [
        "A differenza della base pixel, la base PCA permette di recuperare le caratteristiche salienti dell'immagine in ingresso con solo una media più otto componenti! La quantità di ciascun pixel in ciascun componente è il corollario dell'orientamento del vettore nel nostro esempio bidimensionale. Questo è il senso in cui la PCA fornisce una rappresentazione a bassa dimensionalità dei dati: scopre un insieme di funzioni di base che sono più efficienti della base pixel nativa dei dati di input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sdiWjLxJNTjC"
      },
      "source": [
        "### Scelta del numero di componenti\n",
        "\n",
        "Una parte vitale dell’utilizzo pratico della PCA è la capacità di stimare quanti componenti sono necessari per descrivere i dati. Ciò può essere determinato osservando il rapporto di varianza spiegata cumulativa in funzione del numero di componenti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qt5fxZZzNTjC",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "849a4641-26d7-4fe0-eb22-c5b8f4d3fb9c"
      },
      "outputs": [],
      "source": [
        "pca = PCA().fit(digits.data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "aRDW57tTNTjC"
      },
      "source": [
        "Come già visto, questa curva quantifica quanta parte della varianza totale a 64 dimensioni è contenuta nella prima N componenti. A esempio, vediamo che con le cifre i primi 10 componenti contengono circa il 75% della varianza, mentre sono necessari circa 50 componenti per descrivere quasi il 100% della varianza.\n",
        "\n",
        "Qui vediamo che la nostra proiezione bidimensionale perde molte informazioni (misurate dalla varianza spiegata) e che avremmo bisogno di circa 20 componenti per conservare il 90% della varianza. Osservare questo grafico per un set di dati ad alta dimensione può aiutarci a comprendere il livello di ridondanza presente in più osservazioni."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YakZwUgINTjC"
      },
      "source": [
        "## PCA come filtraggio del rumore\n",
        "\n",
        "La PCA può essere utilizzata anche come approccio di filtraggio per dati rumorosi. L'idea è questa: qualsiasi componente con una varianza molto maggiore dell'effetto del rumore dovrebbe essere relativamente non influenzato dal rumore. Quindi, se ricostruiamo i dati utilizzando solo il sottoinsieme più grande di componenti principali, dovremmo mantenere preferenzialmente il segnale ed eliminare il rumore.\n",
        "\n",
        "Vediamo come appare con i dati in cifre. Per prima cosa tracciamo diversi dati di input privi di rumore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-Mi5xVJINTjC",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "9da4165c-5b79-4ab7-cf28-fb5bc9162e47"
      },
      "outputs": [],
      "source": [
        "def plot_digits(data):\n",
        "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
        "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(data[i].reshape(8, 8),\n",
        "                  cmap='binary', interpolation='nearest',\n",
        "                  clim=(0, 16))\n",
        "plot_digits(digits.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uTHUUB65NTjC"
      },
      "source": [
        "Ora aggiungiamo del rumore casuale per creare un set di dati rumoroso e ritracciarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6h-SACVNTjG",
        "outputId": "b1ce0858-f989-4ec3-e68a-daed14034170"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "rng.normal(10, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "JEi_vsBMNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2ece0d2e-dbc7-4554-dbc8-0cab3f8d1f9f"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "noisy = rng.normal(digits.data, 4)\n",
        "plot_digits(noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bvSmeoFaNTjG"
      },
      "source": [
        "È chiaro a prima vista che le immagini sono rumorose e contengono pixel spuri. Addestriamo una PCA sui dati rumorosi, richiedendo che la proiezione preservi il 50% della varianza:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "prYDxoIONTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "536c548d-f268-4e0c-8be3-c17a2d5a50fb"
      },
      "outputs": [],
      "source": [
        "pca = PCA(0.50).fit(noisy)\n",
        "pca.n_components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4CHUsAIKNTjG"
      },
      "source": [
        "Qui il 50% della varianza ammonta a 12 componenti principali. Ora calcoliamo questi componenti e quindi utilizziamo l'inverso della trasformazione per ricostruire le cifre filtrate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zX_StyPJNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "0fac05ed-de00-4ee6-f5f4-b023397305e5"
      },
      "outputs": [],
      "source": [
        "components = pca.transform(noisy)\n",
        "filtered = pca.inverse_transform(components)\n",
        "plot_digits(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "mrS0nXkTNTjG"
      },
      "source": [
        "Questa proprietà di preservazione del segnale/filtro del rumore rende la PCA una routine di selezione delle funzionalità molto utile: ad esempio, invece di addestrare un classificatore su dati a dimensione molto elevata, potremmo invece addestrare il classificatore sulla rappresentazione a dimensione inferiore, che servirà automaticamente a filtrare rumore casuale negli ingressi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sDb3ruviNTjG"
      },
      "source": [
        "## Esempio\n",
        "\n",
        "Nella spiegazione delle macchine vettoriali di supporto, abbiamo esplorato un esempio di utilizzo di una proiezione PCA come selettore di funzionalità per il riconoscimento facciale, ora daremo uno sguardo indietro ed esploreremo un po’ di più ciò che è successo. Ricordiamo che stavamo utilizzando il set di dati Labeled Faces in the Wild reso disponibile tramite Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4oxzzs8GNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6ec8f664-0602-4103-a4e0-5c272adf1890"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "46mr8o6pNTjG"
      },
      "source": [
        "Diamo un'occhiata agli assi principali che abbracciano questo set di dati. Poiché si tratta di un set di dati di grandi dimensioni, utilizzeremo Randomized PCA che contiene un metodo randomizzato per approssimare il primo N di componenti principali molto più rapidamente dello PCA stimatore standard, e quindi è molto utile per dati ad alta dimensionalità (qui, una dimensionalità di quasi 3.000). Daremo quindi uno sguardo ai primi 150 componenti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6_5qfhXhNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "d74e303a-12b0-49b7-f914-827c8911f3fa"
      },
      "outputs": [],
      "source": [
        "pca = PCA(150, svd_solver='randomized', random_state=42)\n",
        "pca.fit(faces.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NzJ6g8L2NTjG"
      },
      "source": [
        "In questo caso, può essere interessante visualizzare le immagini associate ai primi diversi componenti principali (questi componenti sono tecnicamente noti come \"autovettori\", quindi questi tipi di immagini sono spesso chiamati \"eigenfaces\"). Come possiamo vedere in questa figura, i risultati sono inquietanti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Cel6Z93PNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ae184c98-d15a-4edf-d854-de95b13ce7ca"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yS3y0KnONTjG"
      },
      "source": [
        "I risultati sono molto interessanti e ci danno un'idea di come variano le immagini: a esempio, i primi eigenfaces (dall'alto a sinistra) sembrano essere associati all'angolo di illuminazione sul viso, e successivamente i vettori principali sembrano selezionare alcune caratteristiche, come occhi, naso e labbra. Diamo un'occhiata alla varianza cumulativa di questi componenti per vedere quanta informazione sui dati viene preservata dalla proiezione:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4MTDtBadNTjG",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "fb393dfa-93b6-48a5-abcd-d6bb1e929ea9"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hq9z2hGZNTjH"
      },
      "source": [
        "Vediamo che questi 150 componenti rappresentano poco più del 90% della varianza. Ciò ci porterebbe a credere che utilizzando questi 150 componenti recupereremo la maggior parte delle caratteristiche essenziali dei dati. Per renderlo più concreto, possiamo confrontare le immagini di input con le immagini ricostruite da questi 150 componenti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Zt1kAWvwNTjH",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Compute the components and projected faces\n",
        "pca = pca.fit(faces.data)\n",
        "components = pca.transform(faces.data)\n",
        "projected = pca.inverse_transform(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CwvPHbeONTjH",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "4348e973-b57d-4676-e336-bf1c706eb518"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
        "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
        "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
        "\n",
        "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
        "ax[1, 0].set_ylabel('150-dim\\nreconstruction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "M1hZmNAQNTjH"
      },
      "source": [
        "La riga superiore qui mostra le immagini di input, mentre la riga inferiore mostra la ricostruzione delle immagini da appena 150 delle circa 3.000 caratteristiche iniziali. Questa visualizzazione chiarisce il motivo per cui la selezione delle funzionalità PCA utilizzata nella spiegazione delle Support Vector Machines ha avuto così tanto successo: sebbene riduca la dimensionalità dei dati di quasi un fattore 20, le immagini proiettate contengono informazioni sufficienti che potremmo, a occhio, riconoscere gli individui nell'immagine. Ciò significa che il nostro algoritmo di classificazione deve essere addestrato su dati a 150 dimensioni anziché su dati a 3.000 dimensioni, il che, a seconda del particolare algoritmo che scegliamo, può portare a una classificazione molto più efficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fWkrUpfwNTjH"
      },
      "source": [
        "## Conclusione su PCA\n",
        "\n",
        "In questa sezione abbiamo discusso l'uso dell'analisi delle componenti principali per la riduzione della dimensionalità, per la visualizzazione di dati ad alta dimensionalità, per il filtraggio del rumore e per la selezione delle caratteristiche all'interno di dati ad alta dimensionalità. Grazie alla versatilità e all’interpretabilità della PCA, è stato dimostrato che è efficace in un’ampia varietà di contesti e discipline. Dato qualsiasi set di dati ad alta dimensione, molto spesso conviene utilizzare PCA per visualizzare la relazione tra i punti (come abbiamo fatto con le cifre), per comprendere la varianza principale nei dati (come abbiamo fatto con le eigenfaces) e per comprendere la dimensionalità intrinseca (tracciando il rapporto di varianza spiegato). Certamente la PCA non è utile per ogni set di dati ad alta dimensionalità, ma offre un percorso semplice ed efficiente per ottenere informazioni dettagliate su dati ad alta dimensionalità.\n",
        "\n",
        "### Il principale punto debole della PCA \n",
        "è che tende a essere fortemente influenzato da valori anomali nei dati. Per questo motivo sono state sviluppate molte varianti robuste della PCA, molte delle quali agiscono per scartare in modo iterativo i punti dati che sono scarsamente descritti dai componenti iniziali. Scikit-Learn contiene un paio di varianti interessanti su PCA, inclusi RandomizedPCA e Sparse PCA, entrambi anche nel sottomodulo sklearn.decomposition . Randomized PCA, che abbiamo visto in precedenza, utilizza un metodo non deterministico per approssimare rapidamente i primi componenti principali in dati ad altissima dimensionalità, SparsePCA invece introduce al contempo un termine di regolarizzazione che serve a imporre la scarsità dei componenti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Esercizio 1\n",
        "\n",
        "Utilizzate l'algoritmo k-Nearest Neighbors (k-NN) e PCA (Principal Component Analysis) per la riduzione delle dimensioni sul dataset \"Breast Cancer Wisconsin (Diagnostic)\" disponibile nella libreria scikit-learn.\n",
        "\n",
        "### Passaggi\n",
        "- Caricamento del dataset:\n",
        "\n",
        "1. Caricare il dataset \"Breast Cancer Wisconsin (Diagnostic)\" dalla libreria scikit-learn.\n",
        "2. Esplorare il dataset per comprendere le caratteristiche presenti, i loro tipi e la distribuzione delle classi di output.\n",
        "\n",
        "- Preprocessing dei dati:\n",
        "\n",
        "1. Dividere il dataset in features (variabili indipendenti) e target (variabile dipendente).\n",
        "2. Dividere il dataset in training set e test set utilizzando una proporzione del 70-30.\n",
        "\n",
        "- Standardizzazione dei dati:\n",
        "\n",
        "1. Standardizzare le features utilizzando lo StandardScaler di scikit-learn.\n",
        "\n",
        "- PCA (Principal Component Analysis):\n",
        "\n",
        "1. Utilizzare la PCA per ridurre la dimensionalità del dataset mantenendo il 95% della varianza totale.\n",
        "\n",
        "- Creazione del modello k-NN:\n",
        "\n",
        "1. Creare un modello k-NN specificando il numero di vicini desiderato.\n",
        "\n",
        "- Addestramento del modello:\n",
        "\n",
        "1. Addestrare il modello k-NN sul training set.\n",
        "\n",
        "- Valutazione del modello:\n",
        "\n",
        "1. Valutare le prestazioni del modello utilizzando il test set.\n",
        "2. Calcolare l'accuratezza del modello.\n",
        "3. Visualizzare il report di classificazione che include precision, recall e F1-score per ogni classe.\n",
        "4. Visualizzare la matrice di confusione per valutare le prestazioni del modello."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Esercizio 2\n",
        "\n",
        "Utilizzate l'algoritmo k-Nearest Neighbors (k-NN) e PCA (Principal Component Analysis) per la riduzione delle dimensioni sul dataset \"Fashion MNIST\" disponibile nella libreria scikit-learn.\n",
        "\n",
        "### Passaggi\n",
        "- Caricamento del dataset:\n",
        "\n",
        "1. Caricare il dataset \"Fashion MNIST\" dalla libreria scikit-learn.\n",
        "2. Esplorare il dataset per comprendere le caratteristiche presenti, i loro tipi e la distribuzione delle classi di output.\n",
        "\n",
        "- Preprocessing dei dati:\n",
        "\n",
        "1. Dividere il dataset in features (immagini) e target (etichette dei capi di abbigliamento).\n",
        "2. Dividere il dataset in training set e test set utilizzando una proporzione del 80-20.\n",
        "\n",
        "- Creazione del modello k-NN:\n",
        "\n",
        "1. Creare un modello k-NN specificando il numero di vicini desiderato.\n",
        "\n",
        "- Addestramento del modello:\n",
        "\n",
        "1. Addestrare il modello k-NN sul training set.\n",
        "\n",
        "- Valutazione del modello:\n",
        "\n",
        "1. Valutare le prestazioni del modello utilizzando il test set.\n",
        "2. Calcolare l'accuratezza del modello.\n",
        "3. Visualizzare il report di classificazione che include precision, recall e F1-score per ogni classe.\n",
        "4. Visualizzare la matrice di confusione per valutare le prestazioni del modello.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('3.9.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "513788764cd0ec0f97313d5418a13e1ea666d16d72f976a8acadce25a5af2ffc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
