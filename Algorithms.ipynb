{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d1db89",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Un **Random Forest** è un meta-algoritmo di apprendimento supervisionato basato su alberi decisionali. Combina predizioni di molti alberi costruiti su sotto-campionamenti (*bootstrap*) del dataset e su sottoinsiemi casuali delle caratteristiche. \n",
    "\n",
    "In pratica, ogni albero cresce in maniera indipendente e profonda (forte tendenza ad overfitting), ma introducendo casualità negli input e poi mediando le predizioni si ottiene un modello con varianza molto ridotta. Il risultato è un miglioramento dell’accuratezza rispetto a un singolo albero, grazie alla compensazione degli errori individuali nella fase di aggregazione.\n",
    "\n",
    "## Vantaggi\n",
    "- Riduce l’overfitting dei singoli alberi decisionali (varianza bassa).\n",
    "- Può catturare relazioni non lineari e interazioni complesse tra le variabili.\n",
    "- Funziona bene con dataset di grandi dimensioni.\n",
    "- Gestisce efficacemente dati mancanti e caratteristiche irrilevanti.\n",
    "- È naturalmente parallellizzabile perché ogni albero è indipendente.\n",
    "\n",
    "## Svantaggi\n",
    "- Richiede calcoli intensivi (molti alberi profondi), in particolare su dataset di grandi dimensioni.\n",
    "- Il modello risultante è meno interpretabile di un singolo albero.\n",
    "- Si possono ottenere importanze di caratteristica, ma il meccanismo interno resta complesso e non lineare.\n",
    "- Non performa bene su feature altamente sbilanciate o con pochissimi esempi di alcune classi senza tecniche aggiuntive.\n",
    "\n",
    "## Quando usarlo\n",
    "Adatto per problemi di classificazione o regressione quando si desidera un modello robusto e preciso su dati eterogenei e non lineari. Va bene in presenza di molte variabili e quando vogliamo ridurre l’overfitting senza pre-eliminare feature. Utile anche per valutare importanze di feature dopo la fase di addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571985b",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** è una tecnica di ensemble che costruisce sequenzialmente un modello forte (con alta capacità predittiva) a partire da molti modelli deboli (tipicamente alberi decisionali bassi). Ad ogni iterazione si addestra un nuovo albero sui residui (pseudo-errore) del modello corrente, cercando di minimizzare una funzione di perdita differenziabile. In questo modo, ogni nuovo albero corregge gli errori di quelli precedenti, ottimizzando gradualmente la previsione complessiva. Il risultato è un solo modello complesso composto dalla somma pesata di molti alberi, che in genere sovraperforma metodi più semplici come **Random Forest**.\n",
    "\n",
    "## Vantaggi\n",
    "- **Molto efficace** su dati tabulari, produce alta accuratezza.\n",
    "- Permette di ottimizzare diverse funzioni di costo (es. log-loss, MSE) grazie alla formulazione in termini di discesa del gradiente.\n",
    "- Nella pratica si dimostra spesso più performante di **Random Forest** su molti task.\n",
    "- Adatto sia a problemi di classificazione che di regressione complesse.\n",
    "\n",
    "## Svantaggi\n",
    "- Richiede molta cura negli **iperparametri** e può saturare i dati (*overfitting*) se non si usano tecniche di regolarizzazione (es. learning rate, profondità degli alberi).\n",
    "- L’addestramento è **sequenziale e lento** (a differenza di RF, dove si possono fit paralleli) ed è quindi più costoso computazionalmente.\n",
    "- Il modello finale (centinaia di alberi) è di **scarsa interpretabilità**: come osserva la documentazione scikit-learn, i gradient boosting compongono centinaia di alberi e non sono facilmente interpretabili attraverso l’ispezione visiva.\n",
    "\n",
    "## Quando usarlo\n",
    "Indicato quando la **massima accuratezza** è prioritaria e si ha tempo di addestrare modelli complessi, ad esempio in competizioni di machine learning su dati strutturati. Utile su dataset di medie o grandi dimensioni, soprattutto quando si sospetta che i pattern richiedano molti alberi di regressione che correggono iterativamente gli errori dei precedenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aac2dc",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "**AdaBoost** (Adaptive Boosting) è un meta-algoritmo di boosting che aggiunge sequenzialmente modelli deboli (tipicamente alberi decisionali molto bassi) mettendo pesi maggiori sugli esempi erroneamente classificati. In pratica, si addestra un albero su tutto il dataset, poi si ricalcola il peso di ogni esempio: quelli più difficili (errati) ottengono peso maggiore e vengono enfatizzati nel passo successivo. Il modello finale è una combinazione pesata degli alberi, dove gli alberi successivi “imparano” a correggere gli errori di quelli precedenti.\n",
    "\n",
    "## Vantaggi\n",
    "- Spesso produce un forte classificatore anche a partire da modelli deboli semplici (ad es. decision stump).\n",
    "- È in grado di adattarsi ai pattern dati assegnando più risorse (peso) agli esempi difficili.\n",
    "- Relativamente semplice da implementare (“best out-of-the-box” per molti dataset).\n",
    "- In alcuni casi si dimostra più robusto all’overfitting rispetto ad altri algoritmi, perché converge teoricamente a un modello di elevata capacità.\n",
    "\n",
    "## Svantaggi\n",
    "- Sensibile ai dati rumorosi e agli outlier: enfatizzando i casi classificati male, eventuali errori di etichettatura o rumore possono avere un effetto eccessivo sul modello finale, portando a forte varianza.\n",
    "- La scelta degli iperparametri (numero di iterazioni, learning rate) deve essere ben calibrata.\n",
    "- Rispetto ad algoritmi più avanzati, l’accuratezza complessiva può essere minore se non si usano modelli deboli adeguati.\n",
    "\n",
    "## Quando usarlo\n",
    "Adatto per problemi di classificazione (binaria o multi-classe) di complessità moderata. Va bene con dataset di dimensione ridotta o media, quando si cercano rapidi miglioramenti su modelli semplici (ad esempio come stadio finale in un ensemble). Utile quando si dispone di dati puliti (poco rumore) e si vuole un modello adattivo che focalizza l’attenzione sui campioni difficili da classificare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68062f01",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "\n",
    "La **PCA** è una tecnica lineare di riduzione della dimensionalità che trasforma i dati originali in nuove variabili ortogonali (componenti principali) che catturano la varianza massima. In pratica, si calcola l’autovettore della matrice di covarianza dei dati e si proiettano i dati sui primi assi principali che spiegano la maggior varianza totale. Riducendo il numero di dimensioni ai primi componenti con maggiore varianza, la PCA consente di condensare l’informazione essenziale del dataset in uno spazio di dimensioni inferiori, facilitando visualizzazione e successiva modellazione.\n",
    "\n",
    "## Vantaggi\n",
    "- **Riduzione del rumore e della complessità**: concentra il segnale nelle prime componenti (aspetti ad alta varianza) e scarta componenti di basso segnale.\n",
    "- **Efficienza computazionale**: può migliorare l’efficienza di modelli successivi e ridurre il rischio di overfitting (ad esempio, in regressione con molte variabili altamente correlate).\n",
    "- **Componenti ortogonali**: le componenti principali sono linearmente ortogonali e non correlate, semplificando l’analisi statistica dei dati.\n",
    "- **Visualizzazione**: ampiamente usata per visualizzare dati ad alta dimensione (es. proiettando su due o tre componenti principali).\n",
    "\n",
    "## Svantaggi\n",
    "- **Modello lineare**: non cattura dipendenze non lineari tra variabili.\n",
    "- **Interpretabilità**: i componenti principali possono essere difficili da interpretare perché sono combinazioni dense delle variabili originali.\n",
    "- **Standardizzazione richiesta**: la PCA richiede dati standardizzati e può essere influenzata da outlier (che alterano molto la covarianza).\n",
    "- **Costo computazionale**: l’algoritmo può essere costoso su dataset molto grandi (richiede decomposizione a valori singolari o autovettori, tipicamente O(min(n_samples, n_features)^3) in forma naïve).\n",
    "- **Componenti densamente espressi**: i componenti PCA risultanti sono “densamente espressi” (non-sparsi), rendendo difficile comprenderne l’effettiva interpretazione.\n",
    "\n",
    "## Quando usarlo\n",
    "- **Riduzione delle variabili**: quando si vuole ridurre il numero di variabili mantenendo la maggior parte dell’informazione.\n",
    "- **Pre-processing**: utile prima di un modello supervisionato per eliminare ridondanze e rumore.\n",
    "- **Visualizzazione**: ideale per visualizzare dati multidimensionali (es. plot 2D o 3D dei primi componenti).\n",
    "- **Compressione dati**: ad esempio, in regressione con molte feature correlate è comune usare la PCA per ottenere poche componenti indipendenti (Principal Component Regression).\n",
    "\n",
    "Non è adatta se il rapporto tra variabili è complesso e non lineare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d334e",
   "metadata": {},
   "source": [
    "# ICA (Independent Component Analysis)\n",
    "\n",
    "La **ICA** è una tecnica di scomposizione di segnali che cerca di separare un segnale multivariato in componenti additivi statisticamente indipendenti. Assume che i dati osservati siano combinazioni lineari di fonti indipendenti non gaussiane e cerca la matrice di trasformazione che renda le componenti il più indipendenti possibile. A differenza della PCA, l’ICA non ha come obiettivo primario ridurre la dimensione, ma piuttosto “sciogliere” segnali sovrapposti (per esempio diversi suoni o immagini mixate) attraverso un criterio di indipendenza statistica.\n",
    "\n",
    "## Vantaggi\n",
    "- **Separazione di sorgenti indipendenti**: eccelle nella separazione di sorgenti indipendenti (“Blind Source Separation”). Ad esempio, risolve il noto cocktail party problem separando contemporaneamente voci sovrapposte registrate da più microfoni.\n",
    "- **Identificazione di strutture nascoste**: è in grado di identificare strutture nascoste basate su non gaussianità dei dati, che sfuggono alla PCA (che cattura solo correlazioni lineari e varianza).\n",
    "\n",
    "## Svantaggi\n",
    "- **Ipotesi stringenti**: funziona correttamente solo se le componenti indipendenti obbediscono a ipotesi stringenti (al massimo una gaussiana, segnale rumoroso minimo).\n",
    "- **Pre-processing richiesto**: ICA non include un termine di rumore nel modello, quindi richiede che i dati siano centrati e whitened (es. con PCA preliminare) per essere correttamente applicata.\n",
    "- **Risultato non unico**: l’ordine, la scala e il segno delle componenti estratte possono variare senza cambiare l’indipendenza.\n",
    "- **Costo computazionale**: ICA può richiedere algoritmi iterativi costosi (ad es. FastICA) e può fallire se il numero di fonti indipendenti supera quello delle misure.\n",
    "\n",
    "## Quando usarlo\n",
    "- **Analisi di segnali e immagini**: principalmente in analisi di segnali e immagini dove si cerca di individuare fonti indipendenti nascoste.\n",
    "- **Esempi tipici**:\n",
    "    - Separare tracce audio sovrapposte.\n",
    "    - Elaborare dati EEG/MEG in neuroscienze per identificare segnali cerebrali indipendenti.\n",
    "    - Isolare pattern nascosti in dataset multivariati.\n",
    "\n",
    "Non è di solito usata per la riduzione dimensionale in senso generico, ma specificamente per problemi di source separation in cui le assunzioni sull’indipendenza sono ragionevoli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b2404",
   "metadata": {},
   "source": [
    "# Confronto: Modelli Supervisionati vs Riduzione Dimensionale\n",
    "\n",
    "## Obiettivo Primario\n",
    "I modelli supervisionati (Random Forest, Gradient Boosting, AdaBoost) cercano di predire una variabile target (classe o valore) sfruttando relazioni tra input e output. Gli algoritmi di riduzione dimensionale (PCA, ICA) mirano invece a trasformare i dati di input:\n",
    "- **PCA** riduce la dimensionalità mantenendo la varianza.\n",
    "- **ICA** separa segnali sovrapposti, senza prevedere direttamente alcun target.\n",
    "\n",
    "In breve: i modelli supervisionati vengono usati per classificare/regredire, mentre i metodi di riduzione dimensionale sono utili per pre-elaborazione, esplorazione e compressione dei dati.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretabilità\n",
    "- **Modelli supervisionati**: Gli ensemble di alberi offrono possibilità moderate di interpretazione (es. importanza di feature, dipendenze parziali), ma la logica interna resta complessa e poco esplicita. Ad esempio, i Gradient Boosting composti da centinaia di alberi non sono facilmente interpretabili tramite singoli alberi.\n",
    "- **Riduzione dimensionale**: \n",
    "    - La PCA produce componenti lineari che massimizzano la varianza, ma spesso perdono significato rispetto alle variabili originali. Le componenti risultano dense e difficili da interpretare.\n",
    "    - L’ICA genera componenti indipendenti che possono avere interpretazioni legate a sorgenti fisiche (ad esempio una fonte sonora specifica). Tuttavia, né gli ensemble né i componenti sono intuitivi come un singolo albero decisionale o poche variabili originali.\n",
    "\n",
    "---\n",
    "\n",
    "## Sensibilità al Rumore e a Dati Non Lineari\n",
    "- **Modelli supervisionati**:\n",
    "    - Gli ensemble di alberi gestiscono bene relazioni non lineari e sono relativamente robusti al rumore (grazie al bagging e alla media delle predizioni nei Random Forest).\n",
    "    - AdaBoost è sensibile agli outlier, enfatizzandoli con pesi maggiori.\n",
    "    - Gradient Boosting può modellare interazioni complesse, ma rischia di imparare il rumore se non regolato.\n",
    "- **Riduzione dimensionale**:\n",
    "    - La PCA assume linearità e distribuzione gaussiana: funziona bene con rumore gaussiano, ma può fallire in presenza di rumore non gaussiano o relazioni non lineari.\n",
    "    - L’ICA richiede dati non gaussiani e indipendenti: separa efficacemente fonti indipendenti, ma non gestisce bene dipendenze lineari e può essere disturbata dal rumore se le ipotesi non sono soddisfatte.\n",
    "\n",
    "---\n",
    "\n",
    "## Prestazioni Computazionali\n",
    "- **Modelli supervisionati**:\n",
    "    - Random Forest costruisce molti alberi profondi in parallelo, ma ogni albero è costoso.\n",
    "    - Gradient Boosting costruisce alberi meno profondi in serie, risultando più lento nell’addestramento.\n",
    "- **Riduzione dimensionale**:\n",
    "    - La PCA richiede decomposizione matriciale (SVD o autovettori), con complessità cubica rispetto al numero di dimensioni ridotte. Esistono soluzioni approssimate più veloci.\n",
    "    - L’ICA utilizza algoritmi iterativi (es. FastICA), che possono essere costosi in termini di tempo, soprattutto per molte componenti da stimare.\n",
    "\n",
    "In sintesi:\n",
    "- Gli ensemble boosting richiedono più tempo in addestramento rispetto alla PCA, ma la predizione può essere veloce.\n",
    "- La PCA è rapida nella trasformazione dei dati una volta calcolata la decomposizione, ma può diventare onerosa con centinaia di feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408ca01",
   "metadata": {},
   "source": [
    "# Approfondimenti su Altri Modelli di Machine Learning\n",
    "\n",
    "## LogisticRegression\n",
    "\n",
    "La **Regressione Logistica** è un modello lineare utilizzato per problemi di classificazione, principalmente binaria. Nonostante il nome, non è un algoritmo di regressione, ma stima la probabilità che un campione appartenga a una classe specifica. Lo fa applicando la funzione logistica (o sigmoide) a una combinazione lineare delle feature di input. Il risultato è un valore tra 0 e 1, che viene interpretato come probabilità e poi convertito in una classe tramite una soglia (solitamente 0.5).\n",
    "\n",
    "### Vantaggi\n",
    "- **Semplice e interpretabile**: I coefficienti del modello indicano l'importanza e la direzione (positiva o negativa) dell'influenza di ciascuna feature.\n",
    "- **Veloce ed efficiente**: Richiede poche risorse computazionali sia in fase di addestramento che di predizione.\n",
    "- **Buona baseline**: È spesso il primo modello da provare in un problema di classificazione per stabilire una performance di riferimento.\n",
    "- **Output probabilistico**: Fornisce probabilità di appartenenza a una classe, utili per valutare l'incertezza della predizione.\n",
    "\n",
    "### Svantaggi\n",
    "- **Assunzione di linearità**: Assume che la relazione tra le feature e i log-odds della variabile target sia lineare. Non è in grado di catturare relazioni complesse o non lineari.\n",
    "- **Sensibile a feature correlate**: Può soffrire di multicollinearità, rendendo i coefficienti instabili e difficili da interpretare.\n",
    "- **Richiede feature engineering**: Per modellare relazioni non lineari, è necessario creare manualmente termini polinomiali o di interazione.\n",
    "\n",
    "### Quando usarlo\n",
    "Ideale per problemi di classificazione binaria dove l'interpretabilità è una priorità. Ottimo come modello di baseline rapido e semplice. Adatto a dataset di grandi dimensioni quando si necessita di un modello efficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## KNeighborsClassifier\n",
    "\n",
    "Il **K-Nearest Neighbors (KNN)** è un algoritmo di apprendimento non parametrico e \"lazy\" (pigro). Non costruisce un modello esplicito durante la fase di addestramento, ma memorizza semplicemente l'intero dataset. Per classificare un nuovo punto, cerca i *k* campioni più vicini nel training set (i \"vicini\") e assegna la classe che è più frequente tra di essi (voto di maggioranza).\n",
    "\n",
    "### Vantaggi\n",
    "- **Semplice e intuitivo**: L'idea di base è facile da comprendere e implementare.\n",
    "- **Nessuna assunzione sui dati**: Non fa ipotesi sulla distribuzione dei dati, quindi può gestire confini decisionali complessi e non lineari.\n",
    "- **Si adatta facilmente**: Nuovi dati possono essere aggiunti senza dover riaddestrare il modello.\n",
    "\n",
    "### Svantaggi\n",
    "- **Costo computazionale**: La predizione è lenta su dataset di grandi dimensioni, poiché richiede il calcolo della distanza dal nuovo punto a tutti i punti del training set.\n",
    "- **Maledizione della dimensionalità**: Le prestazioni degradano rapidamente all'aumentare del numero di feature, poiché la nozione di \"vicinanza\" perde significato.\n",
    "- **Sensibile alla scala delle feature**: È necessario standardizzare i dati, altrimenti le feature con scale più grandi domineranno il calcolo della distanza.\n",
    "- **Scelta di *k***: La performance dipende molto dalla scelta del parametro *k* (numero di vicini).\n",
    "\n",
    "### Quando usarlo\n",
    "Adatto per dataset di piccole o medie dimensioni dove i confini decisionali sono irregolari e non facilmente modellabili da algoritmi lineari. Utile quando non si hanno conoscenze a priori sulla distribuzione dei dati.\n",
    "\n",
    "---\n",
    "\n",
    "## SVC (Support Vector Classifier)\n",
    "\n",
    "Le **Support Vector Machines (SVM)** per la classificazione (SVC) sono modelli che cercano di trovare l'iperpiano ottimale che separa le classi nel modo più netto possibile. L'iperpiano \"migliore\" è quello che massimizza il margine, ovvero la distanza tra l'iperpiano stesso e i punti più vicini di ciascuna classe (i \"vettori di supporto\"). Grazie al \"kernel trick\", le SVM possono mappare i dati in uno spazio a dimensione superiore per trovare un separatore lineare anche quando i dati non sono linearmente separabili nello spazio originale.\n",
    "\n",
    "### Vantaggi\n",
    "- **Efficace in spazi ad alta dimensionalità**: Funziona bene anche quando il numero di feature è superiore al numero di campioni.\n",
    "- **Robusto all'overfitting**: La massimizzazione del margine agisce come una forma di regolarizzazione.\n",
    "- **Versatile grazie ai kernel**: Può modellare confini decisionali non lineari complessi (es. con kernel RBF, polinomiale).\n",
    "- **Efficiente in memoria**: Utilizza solo un sottoinsieme dei punti di addestramento (i vettori di supporto) per definire il modello.\n",
    "\n",
    "### Svantaggi\n",
    "- **Costo computazionale**: L'addestramento può essere molto lento su dataset di grandi dimensioni (complessità tra O(n²) e O(n³)).\n",
    "- **Scelta del kernel e dei parametri**: La performance è molto sensibile alla scelta del tipo di kernel e dei suoi iperparametri (es. C, gamma).\n",
    "- **Meno interpretabile**: Il modello risultante, specialmente con kernel non lineari, è una \"black box\".\n",
    "- **Non fornisce stime di probabilità native**: Le probabilità devono essere stimate tramite un processo aggiuntivo e costoso (es. calibrazione di Platt).\n",
    "\n",
    "### Quando usarlo\n",
    "Ottimo per problemi di classificazione con confini decisionali complessi e non lineari, specialmente in spazi ad alta dimensionalità (es. bioinformatica, text classification). Adatto a dataset di medie dimensioni dove l'accuratezza è più importante della velocità di addestramento.\n",
    "\n",
    "---\n",
    "\n",
    "## DecisionTreeClassifier\n",
    "\n",
    "Un **Albero Decisionale** è un modello non parametrico che apprende una serie di regole decisionali (if-then-else) per predire il valore di una variabile target. L'albero viene costruito dividendo ricorsivamente il dataset in sottoinsiemi sempre più puri, basandosi sulla feature che offre il miglior \"split\" (es. massimizzando l'Information Gain o minimizzando l'impurità di Gini).\n",
    "\n",
    "### Vantaggi\n",
    "- **Molto interpretabile**: La struttura ad albero è facile da visualizzare e comprendere. Le regole apprese sono esplicite.\n",
    "- **Richiede poca pre-elaborazione**: Non necessita di standardizzazione delle feature.\n",
    "- **Gestisce dati misti**: Può lavorare con feature sia numeriche che categoriche.\n",
    "- **Cattura interazioni non lineari**: È in grado di modellare relazioni complesse tra le variabili.\n",
    "\n",
    "### Svantaggi\n",
    "- **Tendenza all'overfitting**: Un singolo albero, se non potato, tende a crescere molto in profondità e ad adattarsi perfettamente al rumore del training set, generalizzando male.\n",
    "- **Instabilità**: Piccole variazioni nei dati possono portare a un albero completamente diverso.\n",
    "- **Confini decisionali assiali**: Gli split sono sempre paralleli agli assi delle feature, il che può creare confini decisionali a \"scalini\" poco efficienti per separare dati con relazioni diagonali.\n",
    "\n",
    "### Quando usarlo\n",
    "Quando l'interpretabilità del modello è la massima priorità. Utile per analisi esplorative per capire quali feature sono più importanti e come interagiscono. Spesso usato come componente base per algoritmi di ensemble più potenti.\n",
    "\n",
    "---\n",
    "\n",
    "# Modelli di Regressione\n",
    "\n",
    "## LinearRegression, Ridge e Lasso\n",
    "\n",
    "- **LinearRegression**: È il modello di regressione più semplice. Cerca di trovare la linea (o iperpiano) che meglio si adatta ai dati minimizzando la somma dei quadrati dei residui (Ordinary Least Squares). Assume una relazione lineare tra input e output.\n",
    "- **Ridge Regression**: È una versione regolarizzata della regressione lineare. Aggiunge una penalità L2 (proporzionale al quadrato della magnitudine dei coefficienti) alla funzione di costo. Questo \"restringe\" i coefficienti, riducendo la loro varianza e rendendo il modello più robusto alla multicollinearità.\n",
    "- **Lasso Regression**: Simile a Ridge, ma aggiunge una penalità L1 (proporzionale al valore assoluto dei coefficienti). Un effetto chiave della penalità L1 è che può azzerare completamente i coefficienti delle feature meno importanti, eseguendo così una selezione automatica delle feature.\n",
    "\n",
    "### Vantaggi\n",
    "- **Interpretabilità (soprattutto Lasso)**: I modelli lineari sono facili da interpretare. Lasso evidenzia le feature più rilevanti.\n",
    "- **Efficienza**: Sono computazionalmente molto veloci.\n",
    "- **Regolarizzazione (Ridge, Lasso)**: Riducono l'overfitting e gestiscono la multicollinearità.\n",
    "\n",
    "### Svantaggi\n",
    "- **Assunzione di linearità**: Non possono catturare relazioni non lineari complesse.\n",
    "- **Sensibilità agli outlier**: La minimizzazione dei quadrati degli errori dà molto peso agli outlier.\n",
    "\n",
    "### Quando usarli\n",
    "- **LinearRegression**: Come baseline veloce per problemi di regressione.\n",
    "- **Ridge**: Quando si sospetta multicollinearità tra le feature.\n",
    "- **Lasso**: Quando si hanno molte feature e si vuole un modello più semplice e interpretabile (selezione delle feature).\n",
    "\n",
    "---\n",
    "\n",
    "## SVR (Support Vector Regressor)\n",
    "\n",
    "La **Support Vector Regression (SVR)** è l'analogo delle SVM per i problemi di regressione. Invece di massimizzare il margine tra le classi, la SVR cerca di adattare un iperpiano ai dati in modo che il maggior numero possibile di campioni si trovi all'interno di un \"tubo\" (o margine) definito attorno all'iperpiano. I punti che cadono al di fuori di questo margine vengono penalizzati. Come la SVC, può usare i kernel per modellare relazioni non lineari.\n",
    "\n",
    "### Vantaggi\n",
    "- **Efficace con non linearità**: Grazie ai kernel, può modellare relazioni complesse.\n",
    "- **Robusta agli outlier**: Ignora gli errori all'interno del margine, rendendola meno sensibile a piccoli errori e outlier rispetto ai modelli basati sui minimi quadrati.\n",
    "\n",
    "### Svantaggi\n",
    "- **Lenta su grandi dataset**: Come la SVC, ha un costo computazionale elevato.\n",
    "- **Sensibile agli iperparametri**: Richiede un'attenta calibrazione del kernel e dei parametri (C, epsilon).\n",
    "\n",
    "### Quando usarla\n",
    "Quando si affrontano problemi di regressione con relazioni non lineari e si sospetta la presenza di outlier. Adatta a dataset di medie dimensioni.\n",
    "\n",
    "---\n",
    "\n",
    "## DecisionTreeRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "Questi sono gli analoghi per la regressione dei loro corrispettivi per la classificazione.\n",
    "\n",
    "- **DecisionTreeRegressor**: Costruisce un albero decisionale dove ogni foglia contiene un valore di predizione continuo (solitamente la media dei valori target dei campioni in quella foglia). Soffre degli stessi problemi di overfitting e instabilità del classificatore.\n",
    "- **RandomForestRegressor**: Un ensemble di alberi di regressione addestrati su sottoinsiemi bootstrap dei dati. La predizione finale è la media delle predizioni di tutti gli alberi. Riduce drasticamente l'overfitting e l'instabilità del singolo albero.\n",
    "- **GradientBoostingRegressor**: Costruisce un ensemble di alberi in modo sequenziale. Ogni nuovo albero viene addestrato per correggere gli errori (residui) del modello precedente. È uno degli algoritmi più potenti per dati tabulari.\n",
    "\n",
    "### Vantaggi\n",
    "- **Catturano non linearità complesse**: Tutti e tre possono modellare relazioni molto complesse.\n",
    "- **Robusti (RF, GB)**: Gli ensemble sono molto più stabili e precisi dei singoli alberi.\n",
    "- **Non richiedono scaling**: Gli alberi non sono sensibili alla scala delle feature.\n",
    "\n",
    "### Svantaggi\n",
    "- **Meno interpretabili (RF, GB)**: Un insieme di centinaia di alberi è una \"black box\".\n",
    "- **Rischio di overfitting (GB)**: Il Gradient Boosting può sovradattarsi se non regolato attentamente.\n",
    "- **Costo computazionale (GB)**: L'addestramento sequenziale del Gradient Boosting è lento.\n",
    "\n",
    "### Quando usarli\n",
    "- **DecisionTreeRegressor**: Per modelli interpretabili o come base per capire i dati.\n",
    "- **RandomForestRegressor**: Come modello robusto e \"tuttofare\" che offre buone performance con poca calibrazione.\n",
    "- **GradientBoostingRegressor**: Quando si cerca la massima accuratezza possibile su dati tabulari e si ha tempo per l'addestramento e la calibrazione degli iperparametri."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
