{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5840a6",
   "metadata": {},
   "source": [
    "# Introduzione a scikit-learn\n",
    "\n",
    "Scikit-learn è una popolare libreria Python per il machine learning che offre strumenti facili da usare sia per classificazione che per regressione. In classificazione si assegnano etichette discrete agli esempi (ad esempio riconoscere se un fiore è Iris-setosa o Iris-versicolor) scikit-learn.org, mentre nella regressione si prevedono valori numerici continui (ad esempio il prezzo di una casa in base alle sue caratteristiche)\n",
    "scikit-learn.org. Un esperimento tipico con scikit-learn segue una pipeline di elaborazione dati che include:\n",
    "\n",
    "- Caricamento dei dati (es. dataset integrati come Iris, Wine, Diabetes).\n",
    "- Preprocessing delle feature (ad esempio scaling con StandardScaler).\n",
    "- Divisione in training/test (con train_test_split per ottenere dati indipendenti di addestramento e valutazione)\n",
    "- Definizione del modello (scegliere un classificatore o regressore).\n",
    "- Addestramento del modello usando .fit().\n",
    "- Predizione sui dati di test usando .predict().\n",
    "- Valutazione delle prestazioni (ad esempio usando .score(), accuracy_score, mean_squared_error) \n",
    "\n",
    "Questa sequenza copre i passi fondamentali: addestrare un modello su dati noti e valutarlo su dati nuovi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4260789",
   "metadata": {},
   "source": [
    "# Guida semplice per installare scikit-learn\n",
    "\n",
    "Per installare scikit-learn, puoi utilizzare il gestore di pacchetti `pip` o `conda`, a seconda dell'ambiente Python che stai utilizzando.\n",
    "\n",
    "## Installazione con pip\n",
    "Se stai utilizzando un ambiente Python standard, puoi installare scikit-learn con il seguente comando:\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "## Installazione con conda\n",
    "Se stai utilizzando Anaconda o Miniconda, puoi installare scikit-learn con il comando:\n",
    "```bash\n",
    "conda install -c anaconda scikit-learn\n",
    "```\n",
    "\n",
    "## Verifica dell'installazione\n",
    "Dopo l'installazione, puoi verificare che scikit-learn sia stato installato correttamente eseguendo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88523564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42098a09",
   "metadata": {},
   "source": [
    "Questo comando stamperà la versione di scikit-learn installata nel tuo ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5d5fb",
   "metadata": {},
   "source": [
    "# Configurazione nel contesto dell'Ambiente virtuale\n",
    "\n",
    "## Creazione di un ambiente virtuale\n",
    "Per isolare le dipendenze del progetto, puoi creare un ambiente virtuale utilizzando il seguente comando:\n",
    "```bash\n",
    "python -m venv env\n",
    "```\n",
    "\n",
    "## Attivazione dell'ambiente virtuale\n",
    "Una volta creato l'ambiente virtuale, attivalo con il comando appropriato per il tuo sistema operativo:\n",
    "- **Linux/Mac**: \n",
    "    ```bash\n",
    "    source env/bin/activate\n",
    "    ```\n",
    "- **Windows**: \n",
    "    ```bash\n",
    "    env\\Scripts\\activate\n",
    "    ```\n",
    "\n",
    "## Installazione di scikit-learn\n",
    "Dopo aver attivato l'ambiente virtuale, puoi installare scikit-learn al suo interno utilizzando:\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "## Aggiornamento delle Dipendenze\n",
    "Per aggiornare scikit-learn all'ultima versione disponibile, puoi utilizzare il seguente comando:\n",
    "```bash\n",
    "pip install --upgrade scikit-learn\n",
    "```\n",
    "\n",
    "Se necessario, verifica e aggiorna anche le dipendenze principali come NumPy e SciPy:\n",
    "```bash\n",
    "pip install --upgrade numpy scipy\n",
    "```\n",
    "\n",
    "## Test di Compatibilità\n",
    "Per assicurarti che le versioni di Python e delle librerie siano compatibili, puoi utilizzare i seguenti comandi:\n",
    "\n",
    "### Controllo della versione di Python\n",
    "```bash\n",
    "python --version\n",
    "```\n",
    "\n",
    "### Controllo delle librerie installate\n",
    "Per vedere tutte le librerie installate e le loro versioni, usa:\n",
    "```bash\n",
    "pip list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ced30",
   "metadata": {},
   "source": [
    "# Metodi principali di Scikit-learn\n",
    "\n",
    "## sklearn.datasets: Caricamento e Generazione di Dataset\n",
    "\n",
    "La libreria `sklearn.datasets` offre strumenti per caricare dataset di esempio e generare dataset sintetici. Questi dataset sono utili per testare algoritmi e apprendere come utilizzare scikit-learn.\n",
    "\n",
    "## Dataset di esempio\n",
    "Scikit-learn include diversi dataset integrati che possono essere caricati facilmente. Alcuni esempi comuni includono:\n",
    "- **Iris**: Dataset per la classificazione di fiori.\n",
    "- **Wine**: Dataset per la classificazione di tipi di vino.\n",
    "- **Diabetes**: Dataset per la regressione sul diabete.\n",
    "\n",
    "### Esempio: Caricamento del dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95cfffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carica il dataset Iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Visualizza le chiavi disponibili nel dataset\n",
    "print(iris.keys())\n",
    "\n",
    "# Visualizza le prime 5 righe dei dati\n",
    "print(iris.data[:5])\n",
    "\n",
    "# Visualizza le etichette corrispondenti\n",
    "print(iris.target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ecd95",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset sintetici\n",
    "Scikit-learn permette di generare dataset sintetici per testare algoritmi. Alcuni esempi includono:\n",
    "- **make_classification**: Genera dataset per problemi di classificazione.\n",
    "- **make_regression**: Genera dataset per problemi di regressione.\n",
    "- **make_blobs**: Genera cluster di punti per problemi di clustering.\n",
    "\n",
    "### Esempio: Generazione di un dataset di classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9faf207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 5 righe dei dati:\n",
      "[[ 0.06561344 -1.81431652 -0.84853401  0.27872303 -1.89816218]\n",
      " [ 1.9436449  -0.7812514  -0.78040602 -1.02038325 -0.17664666]\n",
      " [ 2.21543669 -1.59145991 -1.7541757  -0.15772396  0.8755859 ]\n",
      " [ 2.24889649  1.43883123  1.07127592  0.16608813 -0.73306457]\n",
      " [ 1.39888763  1.60328098  1.30729431 -2.4522646  -0.76359582]]\n",
      "Prime 5 etichette:\n",
      "[0 1 1 1 0]\n",
      "\n",
      "Shape del dataset: (100, 5)\n",
      "Numero di classi: 2\n",
      "Distribuzione delle classi: 50 classe 0, 50 classe 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Genera un dataset di classificazione\n",
    "# n_samples: Numero totale di campioni da generare.\n",
    "# n_features: Numero totale di feature (caratteristiche) da generare per ogni campione.\n",
    "# n_informative: Numero di feature informative (devono essere <= n_features)\n",
    "# n_redundant: Numero di feature ridondanti (combinazioni lineari di quelle informative)\n",
    "# n_classes: Numero di classi nel dataset (problema di classificazione).\n",
    "# random_state: Seme per garantire la riproducibilità dei risultati.\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=3, n_redundant=1, n_classes=2, random_state=42)\n",
    "\n",
    "# Visualizza le prime 5 righe dei dati\n",
    "print(\"Prime 5 righe dei dati:\")\n",
    "print(X[:5])\n",
    "\n",
    "# Visualizza le etichette corrispondenti\n",
    "print(\"Prime 5 etichette:\")\n",
    "print(y[:5])\n",
    "\n",
    "# Visualizza informazioni sul dataset\n",
    "print(f\"\\nShape del dataset: {X.shape}\")\n",
    "print(f\"Numero di classi: {len(set(y))}\")\n",
    "print(f\"Distribuzione delle classi: {list(y).count(0)} classe 0, {list(y).count(1)} classe 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df091db",
   "metadata": {},
   "source": [
    "### Esempio: Generazione di un dataset di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4ef8e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime 5 righe dei dati:\n",
      "[[ 0.13074058 -1.43014138 -0.44004449]\n",
      " [ 1.50235705 -0.26940683  0.71754226]\n",
      " [ 0.64768854  0.49671415 -0.1382643 ]\n",
      " [ 0.34175598 -0.75913266  0.15039379]\n",
      " [ 0.85639879 -1.51484722 -0.44651495]]\n",
      "Prime 5 etichette:\n",
      "[-55.34280021 153.84274546  48.31418843  18.03506629  -5.84576917]\n",
      "\n",
      "Shape del dataset: (200, 3)\n",
      "Media delle etichette: -6.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=200, n_features=3, noise=0.2, random_state=42)\n",
    "\n",
    "# Visualizza le prime 5 righe dei dati\n",
    "print(\"Prime 5 righe dei dati:\")\n",
    "print(X[:5])\n",
    "\n",
    "# Visualizza le etichette corrispondenti\n",
    "print(\"Prime 5 etichette:\")\n",
    "print(y[:5])\n",
    "\n",
    "# Visualizza informazioni sul dataset\n",
    "print(f\"\\nShape del dataset: {X.shape}\")\n",
    "print(f\"Media delle etichette: {y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2f929",
   "metadata": {},
   "source": [
    "### Esempio: Generazione di un dataset di clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "607ee6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.72642091 -8.39495682]\n",
      " [ 5.45339605  0.74230537]\n",
      " [-2.97867201  9.55684617]\n",
      " [ 6.04267315  0.57131862]\n",
      " [-6.52183983 -6.31932507]]\n",
      "[2 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Genera un dataset di clustering\n",
    "X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n",
    "\n",
    "# Visualizza le prime 5 righe dei dati\n",
    "print(X[:5])\n",
    "\n",
    "# Visualizza le etichette corrispondenti\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23737ec1",
   "metadata": {},
   "source": [
    "Questi strumenti sono fondamentali per sperimentare e comprendere il funzionamento degli algoritmi di machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## sklearn.preprocessing: Preprocessing dei Dati\n",
    "\n",
    "La libreria 'sklearn.preprocessing' offre strumenti per pre-processare i dati, rendendoli più adatti agli algoritmi di machine learning. Include metodi per scalare, normalizzare, codificare variabili categoriche e gestire dati mancanti.\n",
    "\n",
    "## Scalatura e Normalizzazione\n",
    "La scalatura e la normalizzazione sono tecniche fondamentali per garantire che i dati siano distribuiti uniformemente e abbiano valori comparabili.\n",
    "\n",
    "### Esempio: Standardizzazione con StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62674762 -1.22856868 -1.95030479]\n",
      " [-0.4612102  -0.20527207  0.34388812]\n",
      " [-0.45629324 -0.20220831  0.34457617]\n",
      " [-0.45137629 -0.19914455  0.34526422]\n",
      " [ 1.99562735  1.83519362  0.91657628]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crea dati di esempio\n",
    "import numpy as np\n",
    "data = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9]]) #[-100, -1000, -10000], , [1500, 2000, 2500]\n",
    "\n",
    "# Applica la standardizzazione\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba658c1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Esempio: Normalizzazione con MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df3060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.       0.       0.      ]\n",
      " [0.063125 0.334    0.80024 ]\n",
      " [0.065    0.335    0.80048 ]\n",
      " [0.066875 0.336    0.80072 ]\n",
      " [1.       1.       1.      ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Applica la normalizzazione\n",
    "# Fromula x' = (x - min(x)) / (max(x) - min(x))\n",
    "minmax_scaler = MinMaxScaler()\n",
    "normalized_data = minmax_scaler.fit_transform(data)\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad2398",
   "metadata": {},
   "source": [
    "## Codifica delle Variabili Categoriali\n",
    "La codifica delle variabili categoriche è essenziale per convertire dati non numerici in un formato utilizzabile dagli algoritmi di machine learning.\n",
    "\n",
    "### Esempio: One-Hot Encoding con OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d464f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Crea dati categorici di esempio\n",
    "categories = np.array([['red'], ['blue'], ['green']])\n",
    "\n",
    "# Applica la codifica one-hot\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(categories).toarray()\n",
    "\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192af552",
   "metadata": {},
   "source": [
    "## Gestione dei Dati Mancanti\n",
    "Scikit-learn offre strumenti per gestire i dati mancanti, come l'imputazione.\n",
    "\n",
    "### Esempio: Imputazione con SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d94659be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  2.  7.5]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  9. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Crea dati con valori mancanti\n",
    "data_with_nan = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Applica l'imputazione\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data = imputer.fit_transform(data_with_nan)\n",
    "\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0e2fb",
   "metadata": {},
   "source": [
    "\n",
    "Questi strumenti sono essenziali per preparare i dati prima di applicare algoritmi di machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebec5d6",
   "metadata": {},
   "source": [
    "# sklearn.model_selection: Suddivisione dei Dati e Validazione\n",
    "\n",
    "La libreria `sklearn.model_selection` offre strumenti fondamentali per suddividere i dati, validare i modelli e ottimizzare gli iperparametri. Questi metodi sono essenziali per garantire che i modelli di machine learning siano robusti e generalizzabili.\n",
    "\n",
    "## Suddivisione dei Dati: Training e Test Set\n",
    "\n",
    "La suddivisione dei dati in training e test set è un passaggio cruciale per valutare le prestazioni di un modello. La funzione principale utilizzata è `train_test_split`.\n",
    "\n",
    "### Funzione: train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898df5be",
   "metadata": {},
   "source": [
    "\n",
    "- **X_train, y_train**: Dati di addestramento.\n",
    "- **X_test, y_test**: Dati di test.\n",
    "- **test_size**: Percentuale di dati da utilizzare per il test.\n",
    "- **random_state**: Semina per garantire riproducibilità.\n",
    "\n",
    "---\n",
    "\n",
    "## Validazione Incrociata (Cross-Validation)\n",
    "\n",
    "La validazione incrociata è una tecnica per valutare le prestazioni di un modello su diversi sottoinsiemi di dati. Scikit-learn offre diverse funzioni per implementare la validazione incrociata.\n",
    "\n",
    "### Funzione: cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea08ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di esempio (corretto)\n",
    "(X, y) = make_classification(\n",
    "    n_samples=100, \n",
    "    n_features=4,  # Aumentato il numero di feature totali\n",
    "    n_informative=2,  # Feature informative\n",
    "    n_redundant=1,    # Feature ridondanti\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definisce il modello SVM\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Calcola le prestazioni del modello con validazione incrociata\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(\"Accuracy media:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56295aa4",
   "metadata": {},
   "source": [
    "\n",
    "- **cv**: Numero di suddivisioni (folds) per la validazione.\n",
    "- **scores**: Risultati delle metriche di valutazione per ogni fold.\n",
    "\n",
    "---\n",
    "\n",
    "## Ricerca degli Iperparametri Ottimali\n",
    "\n",
    "La scelta degli iperparametri ottimali è fondamentale per migliorare le prestazioni del modello. Scikit-learn offre due approcci principali: Grid Search e Random Search.\n",
    "\n",
    "### Grid Search\n",
    "La ricerca a griglia esplora tutte le combinazioni possibili di iperparametri specificati.\n",
    "\n",
    "#### Funzione: GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100, \n",
    "    n_features=4,  # Aumentato il numero di feature totali\n",
    "    n_informative=2,  # Feature informative\n",
    "    n_redundant=1,    # Feature ridondanti\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definisce il modello SVM\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Definisce la griglia di iperparametri\n",
    "# Definisce la griglia di iperparametri per la ricerca a griglia.\n",
    "# 'C' controlla il trade-off tra classificazione corretta dei training samples e margine di separazione.\n",
    "# 'kernel' specifica il tipo di funzione kernel da utilizzare per trasformare i dati.\n",
    "# 'linear' indica un kernel lineare, che cerca di separare i dati utilizzando un iperpiano lineare.\n",
    "# 'rbf' indica un kernel Radial Basis Function, che permette di separare i dati in uno spazio di dimensioni superiori utilizzando una funzione non lineare.\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Esegue la ricerca a griglia\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Migliori parametri:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc4331",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Random Search\n",
    "La ricerca casuale esplora un sottoinsieme casuale di combinazioni di iperparametri.\n",
    "\n",
    "#### Funzione: RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di esempio\n",
    "X, y = make_classification(\n",
    "    n_samples=100, \n",
    "    n_features=4,  # Aumentato il numero di feature totali\n",
    "    n_informative=2,  # Feature informative\n",
    "    n_redundant=1,    # Feature ridondanti\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definisce il modello SVM\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Definisce la distribuzione casuale di iperparametri\n",
    "param_dist = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# Esegue la ricerca casuale\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Migliori parametri:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad4a60",
   "metadata": {},
   "source": [
    "\n",
    "- **n_iter**: Numero di combinazioni casuali da testare.\n",
    "\n",
    "---\n",
    "\n",
    "Questi strumenti di `sklearn.model_selection` sono essenziali per garantire che i modelli siano ben addestrati, valutati e ottimizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c578c5",
   "metadata": {},
   "source": [
    "# sklearn.decomposition: Decomposizione e Riduzione della Dimensionalità\n",
    "\n",
    "La libreria **sklearn.decomposition** offre strumenti per la decomposizione e la riduzione della dimensionalità dei dati. Questi metodi sono utili per semplificare dataset complessi, ridurre il rumore e migliorare le prestazioni degli algoritmi di machine learning. Tra i metodi principali troviamo:\n",
    "\n",
    "- **PCA (Principal Component Analysis)**: Riduce la dimensionalità dei dati preservando la massima varianza.\n",
    "- **ICA (Independent Component Analysis)**: Identifica componenti indipendenti nei dati.\n",
    "\n",
    "---\n",
    "\n",
    "## PCA: Principal Component Analysis\n",
    "\n",
    "L'Analisi delle Componenti Principali (PCA) è una tecnica per ridurre la dimensionalità dei dati trasformandoli in un nuovo spazio di coordinate, dove ogni componente principale rappresenta una direzione di massima varianza.\n",
    "\n",
    "### Esempio: Applicazione di PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Crea dati di esempio\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n",
    "\n",
    "# Applica PCA per ridurre la dimensionalità a 1 componente\n",
    "pca = PCA(n_components=1)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Dati originali:\", data)\n",
    "print(\"Dati ridotti:\", reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f3e72",
   "metadata": {},
   "source": [
    "### Visualizzazione della Varianza Spiegata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d87180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la varianza spiegata da ogni componente principale\n",
    "print(\"Varianza spiegata:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073db6b3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ICA: Independent Component Analysis\n",
    "\n",
    "L'Analisi delle Componenti Indipendenti (ICA) è una tecnica per separare segnali indipendenti in un dataset. È spesso utilizzata per l'elaborazione di segnali e immagini.\n",
    "\n",
    "### Esempio: Applicazione di ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "import numpy as np\n",
    "\n",
    "# Crea dati di esempio (mix di segnali)\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Applica ICA per estrarre componenti indipendenti\n",
    "ica = FastICA(n_components=2, random_state=42)\n",
    "independent_components = ica.fit_transform(data)\n",
    "\n",
    "print(\"Componenti indipendenti:\", independent_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83062a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Altri Metodi di Decomposizione\n",
    "\n",
    "### NMF: Non-Negative Matrix Factorization\n",
    "Utilizzato per decomporre dati non negativi, come immagini o dati testuali.\n",
    "\n",
    "#### Esempio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4912e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# Crea dati non negativi di esempio\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Applica NMF\n",
    "nmf = NMF(n_components=2, random_state=42)\n",
    "components = nmf.fit_transform(data)\n",
    "\n",
    "print(\"Componenti NMF:\", components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacd4d7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### TruncatedSVD: Singular Value Decomposition Troncata\n",
    "Utilizzato per ridurre la dimensionalità di dati sparsi, come quelli presenti in applicazioni di NLP.\n",
    "\n",
    "#### Esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Crea dati sparsi di esempio\n",
    "data = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "\n",
    "# Applica TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "reduced_data = svd.fit_transform(data)\n",
    "\n",
    "print(\"Dati ridotti con SVD:\", reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa04df",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Questi metodi di decomposizione sono fondamentali per analizzare e trasformare i dati, rendendoli più gestibili e adatti agli algoritmi di machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feffcba",
   "metadata": {},
   "source": [
    "# sklearn.ensemble: Metodi di Ensemble\n",
    "\n",
    "La libreria **sklearn.ensemble** offre metodi di ensemble che combinano più modelli di base per migliorare le prestazioni predittive. Questi metodi sono particolarmente utili per ridurre il rischio di overfitting e aumentare la robustezza del modello. Tra i principali algoritmi di ensemble troviamo:\n",
    "\n",
    "- **Random Forest**: Combina più alberi decisionali per classificazione o regressione.\n",
    "- **Gradient Boosting**: Costruisce modelli sequenziali ottimizzando gli errori residui.\n",
    "- **AdaBoost**: Combina modelli deboli in modo iterativo per creare un modello forte.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "### Teoria\n",
    "Il Random Forest è un metodo di ensemble basato su alberi decisionali. Combina più alberi decisionali indipendenti, ognuno addestrato su un sottoinsieme casuale dei dati e delle feature. La predizione finale è ottenuta tramite media (per regressione) o voto (per classificazione).\n",
    "\n",
    "### Esempio: Classificazione con Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carica il dataset Iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88beb7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "### Teoria\n",
    "Il Gradient Boosting è un metodo di ensemble che costruisce modelli sequenziali, dove ogni modello cerca di correggere gli errori residui del modello precedente. È particolarmente efficace per problemi complessi e dataset con feature non lineari.\n",
    "\n",
    "### Esempio: Regressione con Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "score = gb.score(X_test, y_test)\n",
    "print(\"R^2 Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202c5f6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "### Teoria\n",
    "L'AdaBoost (Adaptive Boosting) combina modelli deboli (ad esempio alberi decisionali di profondità 1) in modo iterativo, assegnando pesi maggiori agli esempi difficili da classificare. Questo approccio migliora la capacità predittiva del modello.\n",
    "\n",
    "### Esempio: Classificazione con AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carica il dataset Wine\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello AdaBoost\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "accuracy = adaboost.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60b381",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto tra i Metodi\n",
    "\n",
    "| Metodo            | Vantaggi                                                                 | Svantaggi                          |\n",
    "|-------------------|--------------------------------------------------------------------------|------------------------------------|\n",
    "| **Random Forest** | Robusto contro overfitting, facile da interpretare                      | Può essere lento con molti alberi |\n",
    "| **Gradient Boosting** | Elevata accuratezza, adatto a problemi complessi                     | Richiede tuning degli iperparametri |\n",
    "| **AdaBoost**      | Efficace con modelli deboli, semplice da implementare                   | Sensibile ai dati rumorosi         |\n",
    "\n",
    "Questi metodi di ensemble sono fondamentali per migliorare le prestazioni predittive e la robustezza dei modelli di machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fc7cb",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "# sklearn.linear_model: Modelli Lineari per Regressione e Classificazione\n",
    "\n",
    "La libreria **sklearn.linear_model** offre una vasta gamma di modelli lineari per risolvere problemi di regressione e classificazione. I modelli lineari assumono che la relazione tra le variabili indipendenti (feature) e la variabile dipendente (target) sia rappresentata da una funzione lineare.\n",
    "\n",
    "---\n",
    "\n",
    "## Regressione Lineare\n",
    "\n",
    "### Teoria\n",
    "La Regressione Lineare è uno dei modelli più semplici e utilizzati per problemi di regressione. Cerca di trovare una linea (o un piano, in caso di più feature) che minimizzi la somma dei quadrati degli errori tra i valori predetti e quelli osservati.\n",
    "\n",
    "### Esempio: Applicazione della Regressione Lineare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9662c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di Regressione Lineare\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "print(\"Coefficiente:\", lr.coef_)\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"R^2 Score:\", lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0d35c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Regressione Logistica\n",
    "\n",
    "### Teoria\n",
    "La Regressione Logistica è un modello lineare utilizzato per problemi di classificazione. Utilizza una funzione sigmoide per trasformare la somma pesata delle feature in una probabilità, permettendo di classificare gli esempi in categorie.\n",
    "\n",
    "### Esempio: Applicazione della Regressione Logistica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di classificazione con parametri corretti\n",
    "# n_informative=2 e n_redundant=0 per evitare conflitti con n_features=2\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di Regressione Logistica\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "accuracy = log_reg.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6ad01",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "### Teoria\n",
    "La Lasso Regression è una variante della Regressione Lineare che utilizza una penalizzazione L1. Questa penalizzazione tende a ridurre a zero i coefficienti di alcune feature, rendendola utile per la selezione delle feature.\n",
    "\n",
    "### Esempio: Applicazione della Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb428f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "print(\"Coefficiente:\", lasso.coef_)\n",
    "print(\"Intercept:\", lasso.intercept_)\n",
    "print(\"R^2 Score:\", lasso.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f5ab0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "### Teoria\n",
    "La Ridge Regression è una variante della Regressione Lineare che utilizza una penalizzazione L2. Questa penalizzazione riduce l'importanza di feature meno rilevanti, ma non le elimina completamente, rendendola utile per gestire problemi di multicollinearità.\n",
    "\n",
    "### Esempio: Applicazione della Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b31bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "print(\"Coefficiente:\", ridge.coef_)\n",
    "print(\"Intercept:\", ridge.intercept_)\n",
    "print(\"R^2 Score:\", ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f680f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto tra i Modelli\n",
    "\n",
    "| Modello              | Vantaggi                                      | Svantaggi                          |\n",
    "|----------------------|-----------------------------------------------|------------------------------------|\n",
    "| **Linear Regression** | Semplice da interpretare                     | Sensibile a outlier e multicollinearità |\n",
    "| **Logistic Regression** | Adatto per classificazione                  | Non gestisce bene dati non lineari |\n",
    "| **Lasso Regression**  | Selezione automatica delle feature           | Può eliminare troppe feature       |\n",
    "| **Ridge Regression**  | Gestisce la multicollinearità                | Non elimina completamente feature irrilevanti |\n",
    "\n",
    "Questi modelli lineari sono fondamentali per risolvere problemi di regressione e classificazione in modo semplice ed efficace.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7c1d6",
   "metadata": {},
   "source": [
    "# sklearn.svm: Macchine a Vettori di Supporto (SVM)\n",
    "\n",
    "La libreria **sklearn.svm** implementa le Macchine a Vettori di Supporto (SVM), un potente algoritmo di machine learning utilizzato per problemi di classificazione e regressione. Le SVM sono particolarmente efficaci per gestire dati ad alta dimensionalità e problemi complessi con margini chiari di separazione.\n",
    "\n",
    "---\n",
    "\n",
    "## Teoria delle SVM\n",
    "\n",
    "### Classificazione\n",
    "Le SVM cercano di trovare un iperpiano che separi i dati appartenenti a classi diverse con il margine massimo. Se i dati non sono linearmente separabili, le SVM utilizzano il **kernel trick** per trasformare i dati in uno spazio di dimensioni superiori, dove possono essere separati linearmente.\n",
    "\n",
    "### Regressione\n",
    "Le SVM per regressione (SVR) cercano di trovare una funzione che si avvicini il più possibile ai dati, mantenendo un margine di tolleranza (epsilon) intorno alla funzione predetta.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio: Classificazione con SVM\n",
    "\n",
    "### Utilizzo di SVC (Support Vector Classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di classificazione con 2 feature per la visualizzazione\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, \n",
    "                           n_clusters_per_class=1, n_classes=2, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello SVM con kernel lineare\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "accuracy = svc.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93582be",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Visualizzazione del Margine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualizza i dati e il margine\n",
    "def plot_svm_decision_boundary(model, X, y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Crea una griglia per il margine\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Disegna il margine\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='black')\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_decision_boundary(svc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c59ba",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio: Regressione con SVM\n",
    "\n",
    "### Utilizzo di SVR (Support Vector Regressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello SVR con kernel RBF\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "score = svr.score(X_test, y_test)\n",
    "print(\"R^2 Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66043141",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Visualizzazione della Regressione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza i dati e la funzione predetta\n",
    "plt.scatter(X_test, y_test, color='blue', label='Dati reali')\n",
    "plt.scatter(X_test, svr.predict(X_test), color='red', label='Predizioni')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eec87",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel nelle SVM\n",
    "\n",
    "Le SVM supportano diversi tipi di kernel per trasformare i dati:\n",
    "- **Lineare**: Utilizzato per dati separabili linearmente.\n",
    "- **Polinomiale**: Adatto per dati con relazioni non lineari.\n",
    "- **RBF (Radial Basis Function)**: Ideale per dati complessi e non lineari.\n",
    "- **Sigmoid**: Utilizzato in alcune applicazioni specifiche.\n",
    "\n",
    "### Esempio: Utilizzo di Kernel Polinomiale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Genera nuovamente il dataset di classificazione per il kernel polinomiale\n",
    "X_class, y_class = make_classification( n_samples=100, n_features=2, n_redundant=0, n_informative=2, \n",
    "                                        n_clusters_per_class=1, n_classes=2, random_state=42)\n",
    "\n",
    "# Suddivide i dati di classificazione in training e test set\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "# Usa il kernel polinomiale con i dati di classificazione\n",
    "svc_poly = SVC(kernel='poly', degree=3, random_state=42)\n",
    "svc_poly.fit(X_train_class, y_train_class)\n",
    "accuracy_poly = svc_poly.score(X_test_class, y_test_class)\n",
    "print(\"Accuracy con kernel polinomiale:\", accuracy_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6192e26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Vantaggi e Svantaggi delle SVM\n",
    "\n",
    "| Vantaggi                              | Svantaggi                          |\n",
    "|---------------------------------------|------------------------------------|\n",
    "| Efficaci per dati ad alta dimensionalità | Sensibili alla scelta del kernel   |\n",
    "| Robuste contro overfitting            | Non scalano bene con dataset grandi |\n",
    "| Supportano classificazione e regressione | Richiedono tuning degli iperparametri |\n",
    "\n",
    "Le SVM sono strumenti versatili e potenti per risolvere problemi complessi, ma richiedono attenzione nella scelta del kernel e nella configurazione degli iperparametri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5246e",
   "metadata": {},
   "source": [
    "# sklearn.tree: Alberi Decisionali\n",
    "\n",
    "La libreria **sklearn.tree** implementa algoritmi basati su alberi decisionali per problemi di classificazione e regressione. Gli alberi decisionali sono modelli non parametrici che apprendono una serie di regole decisionali gerarchiche per suddividere lo spazio delle feature. Uno dei principali vantaggi degli alberi decisionali è la loro interpretabilità, poiché le decisioni prese dal modello possono essere facilmente visualizzate e comprese.\n",
    "\n",
    "---\n",
    "\n",
    "## Albero Decisionale per la Classificazione\n",
    "\n",
    "### Teoria\n",
    "Un albero decisionale per la classificazione (`DecisionTreeClassifier`) costruisce un modello che predice il valore di una variabile target apprendendo semplici regole decisionali dedotte dalle feature dei dati. L'albero viene costruito suddividendo ricorsivamente il dataset in sottoinsiemi più piccoli, basandosi sul valore di una feature che massimizza la separazione tra le classi (ad esempio, usando l'indice di Gini o l'entropia).\n",
    "\n",
    "### Esempio: Classificazione con DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Carica il dataset Iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di albero decisionale\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy del classificatore ad albero decisionale: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ee835",
   "metadata": {},
   "source": [
    "## Albero Decisionale per la Regressione\n",
    "\n",
    "### Teoria\n",
    "Un albero decisionale per la regressione (`DecisionTreeRegressor`) funziona in modo simile alla sua controparte per la classificazione. Tuttavia, invece di predire una classe, predice un valore continuo. L'albero viene costruito per minimizzare un criterio come l'errore quadratico medio (MSE), e la predizione in una foglia è tipicamente la media dei valori target dei campioni in quella foglia.\n",
    "\n",
    "### Esempio: Regressione con DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera un dataset di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Suddivide i dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello di albero decisionale per la regressione\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello\n",
    "score = reg.score(X_test, y_test)\n",
    "print(f\"R^2 Score del regressore ad albero decisionale: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830ee9",
   "metadata": {},
   "source": [
    "## Visualizzazione dell'Albero\n",
    "\n",
    "La capacità di visualizzare un albero decisionale è uno dei suoi maggiori punti di forza. Scikit-learn offre la funzione `plot_tree` per questo scopo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Esempio:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizza l'albero decisionale addestrato per la classificazione\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b240f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vantaggi e Svantaggi degli Alberi Decisionali\n",
    "\n",
    "| Vantaggi                                      | Svantaggi                                       |\n",
    "|-----------------------------------------------|-------------------------------------------------|\n",
    "| **Facili da interpretare e visualizzare**     | **Tendenza all'overfitting**, specialmente con alberi profondi. |\n",
    "| **Richiedono poca preparazione dei dati** (es. non necessitano di scaling). | **Instabili**: piccole variazioni nei dati possono generare alberi molto diversi. |\n",
    "| **Possono gestire dati numerici e categorici** | Le predizioni sono costanti a tratti, non continue. |\n",
    "| **Modello \"White Box\"**: le decisioni sono trasparenti. | Possono creare alberi distorti se alcune classi dominano. |\n",
    "\n",
    "Gli alberi decisionali sono un blocco di costruzione fondamentale per algoritmi di ensemble più potenti come Random Forest e Gradient Boosting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afd920",
   "metadata": {},
   "source": [
    "# sklearn.cluster: Algoritmi di Clustering\n",
    "\n",
    "La libreria **sklearn.cluster** fornisce una suite di algoritmi per il clustering non supervisionato, il cui scopo è raggruppare dati non etichettati in base alla loro somiglianza. Questi metodi sono fondamentali per l'analisi esplorativa dei dati, poiché permettono di scoprire strutture intrinseche, come gruppi o sottogruppi, senza la necessità di informazioni preesistenti sulle categorie. Tra i principali algoritmi troviamo:\n",
    "\n",
    "-   **K-Means**: Partiziona i dati in un numero predefinito di cluster (K) a forma sferica.\n",
    "-   **Agglomerative Clustering**: Costruisce una gerarchia di cluster in modo bottom-up.\n",
    "-   **DBSCAN**: Raggruppa i dati basandosi sulla densità, identificando cluster di forma arbitraria e outlier.\n",
    "\n",
    "---\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "### Teoria\n",
    "L'algoritmo K-Means mira a partizionare `n` osservazioni in `k` cluster, in cui ogni osservazione appartiene al cluster con il centroide (media) più vicino. Funziona in modo iterativo per minimizzare la varianza all'interno di ciascun cluster (inerzia).\n",
    "\n",
    "### Esempio: Applicazione di K-Means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b813d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Genera dati di esempio\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Crea e addestra il modello K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Visualizza i risultati\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cluster centers:\\n\", centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6b86f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Agglomerative Clustering\n",
    "\n",
    "### Teoria\n",
    "L'Agglomerative Clustering è un approccio di clustering gerarchico che costruisce una gerarchia di cluster partendo da singoli punti. Ad ogni passo, unisce le due coppie di cluster più vicine secondo una metrica di linkage (es. `ward`, `complete`, `average`).\n",
    "\n",
    "### Esempio: Applicazione dell'Agglomerative Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02782c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Crea e addestra il modello\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "y_agg = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Visualizza i risultati\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_agg, s=50, cmap='plasma')\n",
    "plt.title('Agglomerative Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca9b8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "### Teoria\n",
    "DBSCAN raggruppa i punti che sono densamente concentrati, contrassegnando come outlier i punti che si trovano in regioni a bassa densità. È particolarmente efficace per identificare cluster di forma non sferica e per gestire il rumore nei dati. I parametri chiave sono `eps` (la distanza massima tra due campioni per essere considerati vicini) e `min_samples` (il numero minimo di campioni in un intorno per formare un cluster).\n",
    "\n",
    "### Esempio: Applicazione di DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Crea e addestra il modello DBSCAN\n",
    "dbscan = DBSCAN(eps=0.6, min_samples=5)\n",
    "y_dbscan = dbscan.fit_predict(X)\n",
    "\n",
    "# Visualizza i risultati\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, s=50, cmap='cividis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Stampa il numero di cluster trovati (escludendo il rumore, etichettato come -1)\n",
    "n_clusters_ = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
    "print(f'Numero di cluster stimati: {n_clusters_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fcef0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Confronto tra i Metodi di Clustering\n",
    "\n",
    "| Metodo | Vantaggi | Svantaggi |\n",
    "| :--- | :--- | :--- |\n",
    "| **K-Means** | Veloce e scalabile per grandi dataset. | Richiede di specificare il numero di cluster (K); assume cluster sferici. |\n",
    "| **Agglomerative Clustering** | Non assume una forma specifica per i cluster; produce una gerarchia utile. | Computazionalmente intensivo, specialmente per grandi dataset. |\n",
    "| **DBSCAN** | Può trovare cluster di forma arbitraria; robusto agli outlier. | Sensibile ai parametri `eps` e `min_samples`; non performa bene con densità variabili. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e44c7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# sklearn.metrics: Valutazione delle Prestazioni del Modello\n",
    "\n",
    "La libreria **sklearn.metrics** è una componente cruciale di Scikit-learn che fornisce un insieme completo di strumenti per valutare le prestazioni dei modelli di machine learning. La scelta della metrica corretta è fondamentale, poiché definisce il criterio con cui si misura il \"successo\" di un modello e dipende strettamente dal tipo di problema che si sta affrontando (classificazione, regressione, clustering).\n",
    "\n",
    "---\n",
    "\n",
    "## Metriche di Classificazione\n",
    "\n",
    "Per i problemi di classificazione, le metriche valutano quanto bene un modello è in grado di assegnare le etichette corrette ai dati.\n",
    "\n",
    "### Teoria\n",
    "-   **Accuracy**: La proporzione di predizioni corrette sul totale. È la metrica più semplice, ma può essere fuorviante in caso di dataset sbilanciati.\n",
    "-   **Precision**: Misura la proporzione di veri positivi tra tutte le predizioni positive. È importante quando il costo dei falsi positivi è alto.\n",
    "-   **Recall (Sensitivity)**: Misura la proporzione di veri positivi identificati correttamente. È cruciale quando il costo dei falsi negativi è alto.\n",
    "-   **F1-Score**: La media armonica di Precision e Recall, fornisce un singolo punteggio che bilancia entrambi.\n",
    "-   **Matrice di Confusione**: Una tabella che riassume le prestazioni di un classificatore, mostrando veri positivi, veri negativi, falsi positivi e falsi negativi.\n",
    "-   **Curva ROC e AUC**: La curva ROC (Receiver Operating Characteristic) visualizza il trade-off tra il tasso di veri positivi e il tasso di falsi positivi. L'AUC (Area Under the Curve) riassume questa curva in un singolo valore, che indica la capacità del modello di distinguere tra le classi.\n",
    "\n",
    "### Esempio: Valutazione di un Classificatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8444872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.74\n",
      "Recall: 0.79\n",
      "F1 Score: 0.76\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       160\n",
      "           1       0.74      0.79      0.76       140\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.77      0.77      0.77       300\n",
      "weighted avg       0.77      0.77      0.77       300\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Genera dati di classificazione\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Addestra un modello\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcola le metriche\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\\n\")\n",
    "\n",
    "# Report di classificazione completo\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b926df9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Metriche di Regressione\n",
    "\n",
    "Per i problemi di regressione, le metriche valutano quanto le predizioni del modello si avvicinano ai valori numerici reali.\n",
    "\n",
    "### Teoria\n",
    "-   **Mean Absolute Error (MAE)**: La media delle differenze assolute tra valori predetti e reali. È facile da interpretare e meno sensibile agli outlier.\n",
    "-   **Mean Squared Error (MSE)**: La media dei quadrati delle differenze tra valori predetti e reali. Penalizza maggiormente gli errori grandi.\n",
    "-   **R-squared (R²)**: Il coefficiente di determinazione. Indica la proporzione della varianza nella variabile dipendente che è prevedibile dalle variabili indipendenti. Un valore vicino a 1 indica una buona aderenza del modello ai dati.\n",
    "\n",
    "### Esempio: Valutazione di un Regressore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4bae862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 8.42\n",
      "Mean Squared Error (MSE): 104.20\n",
      "R-squared (R²): 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Genera dati di regressione\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Addestra un modello\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcola le metriche\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0deb6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Metriche di Clustering\n",
    "\n",
    "Per il clustering, le metriche valutano la qualità dei cluster formati, spesso basandosi sulla compattezza e sulla separazione tra i cluster.\n",
    "\n",
    "### Teoria\n",
    "-   **Silhouette Score**: Misura quanto un oggetto è simile al proprio cluster rispetto agli altri cluster. Varia da -1 a 1, dove un valore alto indica che l'oggetto è ben abbinato al proprio cluster e mal abbinato ai cluster vicini.\n",
    "-   **Adjusted Rand Index (ARI)**: Misura la somiglianza tra le etichette vere e quelle predette, correggendo per l'assegnazione casuale. Richiede la conoscenza delle etichette reali.\n",
    "\n",
    "### Esempio: Valutazione di un Modello di Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c340d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.68\n",
      "Adjusted Rand Index (ARI): 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Genera dati per il clustering\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Addestra un modello\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Calcola le metriche\n",
    "silhouette = silhouette_score(X, y_pred)\n",
    "ari = adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57ac11",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# sklearn.neighbors: Algoritmi Basati sui Vicini\n",
    "\n",
    "La libreria **sklearn.neighbors** implementa algoritmi di apprendimento basati sui vicini (neighbor-based), noti anche come metodi non parametrici. Questi algoritmi basano le loro predizioni sulla somiglianza tra i campioni, senza fare assunzioni sulla distribuzione dei dati. I principali metodi includono:\n",
    "\n",
    "-   **K-Nearest Neighbors (KNN)**: Utilizzato sia per la classificazione che per la regressione, predice l'etichetta di un nuovo punto basandosi sulle etichette dei suoi vicini più prossimi.\n",
    "-   **NearestNeighbors**: Un algoritmo non supervisionato per trovare i vicini più prossimi di ogni punto in un dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## K-Nearest Neighbors (KNN) per la Classificazione\n",
    "\n",
    "### Teoria\n",
    "L'algoritmo **KNeighborsClassifier** classifica un nuovo punto di dati assegnandogli la classe più comune tra i suoi `k` vicini più prossimi nel training set. La \"vicinanza\" è tipicamente misurata con una metrica di distanza, come la distanza euclidea. La scelta di `k` è cruciale: un `k` piccolo può portare a un modello sensibile al rumore, mentre un `k` grande può rendere i confini decisionali meno distinti.\n",
    "\n",
    "### Esempio: Classificazione con KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b21a82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del classificatore KNN: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Genera dati di esempio\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello KNN\n",
    "# n_neighbors (k) è il numero di vicini da considerare\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Esegui le predizioni e valuta il modello\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy del classificatore KNN: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078cf93b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## K-Nearest Neighbors (KNN) per la Regressione\n",
    "\n",
    "### Teoria\n",
    "In modo simile alla classificazione, **KNeighborsRegressor** predice un valore continuo per un nuovo punto di dati. Tuttavia, invece di usare il voto di maggioranza, calcola la media (o una media pesata) dei valori target dei suoi `k` vicini più prossimi.\n",
    "\n",
    "### Esempio: Regressione con KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c42177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score del regressore KNN: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Genera dati di esempio\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea e addestra il modello KNN per la regressione\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=7)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Esegui le predizioni e valuta il modello\n",
    "y_pred = knn_reg.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"R² Score del regressore KNN: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc456b0a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Trovare i Vicini più Prossimi\n",
    "\n",
    "### Teoria\n",
    "L'algoritmo **NearestNeighbors** non è un modello predittivo, ma uno strumento per interrogare un dataset e trovare i vicini più prossimi per uno o più punti. È utile in molti contesti, come l'analisi esplorativa o come pre-processing per altri algoritmi.\n",
    "\n",
    "### Esempio: Uso di NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ea52a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indici dei vicini: [[0 3 1]\n",
      " [4 5 3]]\n",
      "Distanze dai vicini: [[1.41421356 1.41421356 2.23606798]\n",
      " [0.70710678 0.70710678 1.58113883]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Dati di esempio\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "# Crea e addestra il modello\n",
    "# n_neighbors=3 significa che cercheremo i 3 vicini più prossimi (incluso il punto stesso)\n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X)\n",
    "\n",
    "# Trova i vicini per i punti [[0, 0], [2.5, 1.5]]\n",
    "distances, indices = nbrs.kneighbors([[0, 0], [2.5, 1.5]])\n",
    "\n",
    "print(\"Indici dei vicini:\", indices)\n",
    "print(\"Distanze dai vicini:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd71eb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Vantaggi e Svantaggi degli Algoritmi Basati sui Vicini\n",
    "\n",
    "| Vantaggi | Svantaggi |\n",
    "| :--- | :--- |\n",
    "| **Semplice e intuitivo**: Facile da capire e implementare. | **Computazionalmente costoso**: La predizione richiede il calcolo delle distanze da tutti i punti del training set. |\n",
    "| **Nessuna fase di addestramento (Lazy Learning)**: Il \"lavoro\" viene fatto solo al momento della predizione. | **Sensibile alla \"Maledizione della Dimensionalità\"**: Le prestazioni degradano rapidamente all'aumentare del numero di feature. |\n",
    "| **Flessibile**: Può catturare confini decisionali complessi e non lineari. | **Sensibile alla scala delle feature**: È necessario normalizzare i dati. |\n",
    "| **Efficace con dati a bassa dimensionalità**. | **Richiede la scelta ottimale di `k`** e della metrica di distanza. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399341b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# sklearn.pipeline: Costruzione di Pipeline di Machine Learning\n",
    "\n",
    "La libreria **sklearn.pipeline** offre strumenti per concatenare più passaggi di elaborazione (trasformatori) e un modello finale (estimatore) in un unico oggetto. Questo approccio, noto come \"pipeline\", semplifica il flusso di lavoro, previene errori comuni e rende il codice più pulito e riproducibile.\n",
    "\n",
    "---\n",
    "\n",
    "## Teoria della Pipeline\n",
    "\n",
    "Una pipeline in Scikit-learn è un oggetto che incapsula una sequenza di trasformazioni dei dati e un estimatore finale. Quando si chiama il metodo `.fit()` sulla pipeline, i dati vengono passati attraverso ogni passaggio in sequenza: ogni trasformatore viene addestrato (`fit_transform`) sui dati di input e l'output viene passato al passaggio successivo, fino a raggiungere l'estimatore finale, che viene addestrato sui dati trasformati.\n",
    "\n",
    "I principali vantaggi dell'utilizzo di una pipeline sono:\n",
    "-   **Convenienza**: Raggruppa più passaggi in un unico oggetto, semplificando l'addestramento e la valutazione.\n",
    "-   **Prevenzione del Data Leakage**: Garantisce che i passaggi di pre-processing (come la scalatura) vengano addestrati solo sui dati di training, evitando che informazioni del set di test \"trapelino\" nel processo di addestramento.\n",
    "-   **Ottimizzazione degli Iperparametri**: Facilita la ricerca a griglia (Grid Search) simultanea degli iperparametri di tutti i passaggi della pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio: Creazione di una Pipeline di Classificazione\n",
    "\n",
    "In questo esempio, creeremo una pipeline che esegue due passaggi:\n",
    "1.  **Standardizzazione dei dati**: Utilizza `StandardScaler` per scalare le feature.\n",
    "2.  **Classificazione**: Utilizza un classificatore `SVC` (Support Vector Classifier) per effettuare le predizioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7dcf67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy della pipeline: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Genera dati di esempio\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# 2. Definisci i passaggi della pipeline\n",
    "# Ogni passaggio è una tupla ('nome_passaggio', oggetto_stimatore)\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='linear', random_state=42))\n",
    "]\n",
    "\n",
    "# 3. Crea la pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# 4. Addestra la pipeline\n",
    "# StandardScaler viene addestrato e trasforma i dati, poi SVC viene addestrato sui dati trasformati\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 5. Esegui le predizioni\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# 6. Valuta il modello\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy della pipeline: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc44bf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline e Grid Search\n",
    "\n",
    "Uno dei maggiori vantaggi delle pipeline è la loro integrazione con `GridSearchCV` per l'ottimizzazione degli iperparametri. È possibile definire una griglia di parametri per i diversi passaggi della pipeline, specificando il nome del passaggio seguito da `__` e il nome del parametro.\n",
    "\n",
    "### Esempio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6848f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migliori parametri trovati: {'svc__C': 0.1, 'svc__kernel': 'linear'}\n",
      "Migliore accuracy con Grid Search: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisci la griglia di parametri per la pipeline\n",
    "# 'svc__C' si riferisce al parametro C del passaggio 'svc'\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Crea e addestra GridSearchCV con la pipeline\n",
    "search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Migliori parametri trovati:\", search.best_params_)\n",
    "print(f\"Migliore accuracy con Grid Search: {search.best_score_:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
