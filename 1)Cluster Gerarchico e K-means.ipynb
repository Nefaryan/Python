{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering gerarchico\n",
        "Come abbiamo visto, il clustering gerarchico è un metodo di apprendimento non supervisionato per il clustering di punti dati, l'algoritmo crea cluster misurando le differenze tra i dati. Sappiamo che l'apprendimento non supervisionato si basa su un modello che non deve essere addestrato e non abbiamo bisogno di una variabile \"target\", questo metodo può essere utilizzato su qualsiasi dato per visualizzare e interpretare la relazione tra i singoli punti dati.\n",
        "\n",
        "In questa prima parte pratica utilizzeremo il clustering gerarchico per raggruppare i punti dati e visualizzare i cluster utilizzando sia un dendrogramma che un grafico a dispersione.\n",
        "\n",
        "### Come funziona?\n",
        "Utilizzeremo l'Agglomerative Clustering, un tipo di clustering gerarchico che segue un approccio dal basso verso l'alto, iniziamo trattando ciascun punto dati come un proprio cluster, quindi, uniamo insieme i cluster che hanno la distanza più breve tra loro per creare cluster più grandi. Questo passaggio viene ripetuto finché non viene formato un cluster di grandi dimensioni contenente tutti i punti dati.\n",
        "\n",
        "Il clustering gerarchico ci impone di decidere sia il metodo della distanza che quello del collegamento. Utilizzeremo la distanza euclidea e il metodo del collegamento di Ward, che tenta di minimizzare la varianza tra i cluster.\n",
        "\n",
        "### Esempio\n",
        "Iniziamo visualizzando alcuni punti dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\n",
        "y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ora trasformiamo i dati in un insieme di punti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = list(zip(x, y))\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ora grazie alla libreria scipy calcoliamo il collegamento tra tutti i diversi punti. Qui utilizziamo una semplice misura di distanza euclidea e il collegamento di Ward, che cerca di ridurre al minimo la varianza tra i cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "linkage_data = linkage(data, method='ward', metric='euclidean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infine, tracciamo i risultati in un dendrogramma, questo grafico ci mostrerà la gerarchia dei cluster dal basso (singoli punti) verso l'alto (un singolo cluster costituito da tutti i punti dati).\n",
        "\n",
        "plt.show()ci consente di visualizzare il dendrogramma anziché solo i dati grezzi di collegamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dendrogram(linkage_data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La libreria scikit-learn ci consente di utilizzare il clustering gerarchico in modo diverso. Innanzitutto, inizializziamo la classe AgglomerativeClustering con 2 cluster, utilizzando la stessa distanza euclidea e il collegamento di Ward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hierarchical_cluster = AgglomerativeClustering(n_clusters=2, linkage='ward')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il metodo .fit_predict può essere richiamato sui nostri dati per calcolare i cluster utilizzando i parametri definiti nel numero di cluster scelto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = hierarchical_cluster.fit_predict(data) \n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infine, se tracciamo gli stessi dati e coloriamo i punti utilizzando le etichette assegnate a ciascun indice tramite il metodo del clustering gerarchico, possiamo vedere il cluster a cui è stato assegnato ciascun punto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(x, y, c=labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1UHpAbG875"
      },
      "source": [
        "## k-Means Clustering\n",
        "Come abbiamo visto, il K-means è un metodo di apprendimento non supervisionato per il clustering di punti dati, l'algoritmo divide iterativamente i punti dati in K cluster riducendo al minimo la varianza in ciascun cluster.\n",
        "\n",
        "Ora vedremo come stimare il valore migliore per K utilizzando il metodo del gomito, quindi utilizzeremo il clustering K-means per raggruppare i punti dati in cluster.\n",
        "\n",
        "### Come funziona?\n",
        "Innanzitutto, ciascun punto dati viene assegnato in modo casuale a uno dei K cluster. Quindi, calcoliamo il centroide (funzionalmente il centro) di ciascun cluster e riassegniamo ciascun punto dati al cluster con il centroide più vicino. Ripetiamo questo processo finché le assegnazioni dei cluster per ciascun punto dati non cambiano più.\n",
        "\n",
        "k-Means Clustering richiede di selezionare K, il numero di cluster in cui vogliamo raggruppare i dati. Il metodo del gomito ci consente di rappresentare graficamente l'inerzia (una metrica basata sulla distanza) e visualizzare il punto in cui inizia a diminuire linearmente. Questo punto è denominato \"soffio\" ed è una buona stima del miglior valore di K in base ai nostri dati.\n",
        "\n",
        "### Esempio\n",
        "Iniziamo visualizzando alcuni punti dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]\n",
        "y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ora trasformiamo i dati in un insieme di punti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = list(zip(x, y))\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per trovare il valore migliore per K, dobbiamo eseguire le medie K sui nostri dati per un intervallo di valori possibili. Abbiamo solo 10 punti dati, quindi il numero massimo di cluster è 10. Quindi, per ogni valore K in range(1,11), addestriamo un modello K-means e tracciamo l'intertia in corrispondenza di quel numero di cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "inertias = []\n",
        "\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i)\n",
        "    kmeans.fit(data)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1,11), inertias, marker='o')\n",
        "plt.title('Elbow method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Possiamo vedere che il \"gomito\" nel grafico sopra (dove l'interia diventa più lineare) è a K=2. Possiamo quindi adattare ancora una volta il nostro algoritmo delle medie K e tracciare i diversi cluster assegnati ai dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(data)\n",
        "\n",
        "plt.scatter(x, y, c=kmeans.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAruMT3PG87-"
      },
      "source": [
        "## Presentazione più specifica del k-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22fKay9cG87-"
      },
      "source": [
        "Come già detto, l'algoritmo k-means cerca un numero predeterminato di cluster all'interno di un set di dati multidimensionale senza etichetta, per raggiungere questo obiettivo, utilizza una semplice concezione di come si presenta il clustering ottimale:\n",
        "\n",
        "- Il \"centro del cluster\" è la media aritmetica di tutti i punti appartenenti al cluster.\n",
        "- Ogni punto è più vicino al proprio centro del cluster che ad altri centri del cluster.\n",
        "\n",
        "Queste due ipotesi sono alla base del modello k-means, diamo quindi un'occhiata a un semplice set di dati e vediamo il risultato k-means.\n",
        "\n",
        "Innanzitutto, generiamo un set di dati bidimensionale contenente quattro BLOB distinti. Per sottolineare che si tratta di un algoritmo non supervisionato, lasceremo le etichette fuori dalla visualizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ViG8JRKG87-",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "67fc3814-05ea-47be-c1b8-46d9f45b7671"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#make blobs\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN7E4kRmG87_"
      },
      "source": [
        "A occhio è relativamente facile individuare i quattro cluster. L' algoritmo k -means lo fa automaticamente e in Scikit-Learn utilizza la tipica API di stima:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS2iPZPRG87_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYqxwo5PG88A"
      },
      "source": [
        "Visualizziamo i risultati tracciando i dati colorati da queste etichette. Tracciamo anche i centri dei cluster come determinati dallo stimatore k -means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17YQTIqFG88A",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ea5b662a-f9cb-41f5-9746-c8b81567cc1d"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJfXdE08G88B"
      },
      "source": [
        "La buona notizia è che l' algoritmo k-means (almeno in questo caso semplice) assegna i punti ai cluster in modo molto simile a come potremmo assegnarli a occhio. Ma potresti chiederti come fa questo algoritmo a trovare questi cluster così velocemente! Dopotutto, il numero di possibili combinazioni di assegnazioni di cluster è esponenziale nel numero di punti dati: una ricerca esaustiva sarebbe molto, molto costosa. Fortunatamente per noi, una ricerca così esaustiva non è necessaria: invece, l’approccio tipico alle k-means implica un approccio iterativo intuitivo noto come massimizzazione delle aspettative ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soFc10YSG88B"
      },
      "source": [
        "## Massimizzazione dell'aspettativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KErd9id_G88B"
      },
      "source": [
        "In inglese Expectation–maximization (E–M) è un potente algoritmo che emerge in una varietà di contesti nell'ambito della scienza dei dati. k-means è un'applicazione dell'algoritmo particolarmente semplice e di facile comprensione e la esamineremo brevemente qui. In breve, l’approccio di massimizzazione delle aspettative come abbiamo visto consiste nella seguente procedura:\n",
        "\n",
        "- Indovinare alcuni centri cluster\n",
        "- Ripetere fino alla convergenza\n",
        " - E-Step : assegnare i punti al centro del cluster più vicino\n",
        " - M-Step : impostare i centri dei cluster sulla media\n",
        "\n",
        "Qui il \"fase E\" o \"fase delle aspettative\" è chiamato così perché implica l'aggiornamento delle nostre aspettative su a quale cluster appartiene ciascun punto. Il \"passo M\" o \"passo di massimizzazione\" è così chiamato perché implica la massimizzazione di alcune funzioni di fit che definiscono la posizione dei centri dei cluster: in questo caso, tale massimizzazione viene ottenuta prendendo una media semplice dei dati in ciascun cluster .\n",
        "\n",
        "La letteratura su questo algoritmo è vasta, ma può essere riassunta come segue: in circostanze tipiche, ogni ripetizione dell’E-step e dell’M-step risulterà sempre in una migliore stima delle caratteristiche del cluster.\n",
        "\n",
        "Possiamo visualizzare l'algoritmo come mostrato nella figura seguente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsCRgdz7G88B"
      },
      "source": [
        "![k-meas](dati/img/k-means.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7J_nonMG88C"
      },
      "source": [
        "L' algoritmo k-Means è abbastanza semplice da poterlo scrivere in poche righe di codice. Quella che segue è un'implementazione molto semplice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Jh335kG88C",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5fb88b32-d25e-407d-c74e-18c8c3ff55e3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "\n",
        "def find_clusters(X, n_clusters, rseed=2):\n",
        "    # 1. Randomly choose clusters\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
        "    centers = X[i]\n",
        "\n",
        "    while True:\n",
        "        # 2a. Assign labels based on closest center\n",
        "        labels = pairwise_distances_argmin(X, centers)\n",
        "\n",
        "        # 2b. Find new centers from means of points\n",
        "        new_centers = np.array([X[labels == i].mean(0)\n",
        "                                for i in range(n_clusters)])\n",
        "\n",
        "        # 2c. Check for convergence\n",
        "        if np.all(centers == new_centers):\n",
        "            break\n",
        "        centers = new_centers\n",
        "\n",
        "    return centers, labels\n",
        "\n",
        "centers, labels = find_clusters(X, 4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdY5SsUKG88C"
      },
      "source": [
        "La maggior parte delle implementazioni ben testate faranno qualcosa di più dietro le quinte, ma la funzione precedente fornisce l'essenza dell'approccio di massimizzazione delle aspettative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvDiob4tG88C"
      },
      "source": [
        "### Avvertenze sull’aspettativa- massimizzazione\n",
        "\n",
        "Ci sono alcuni problemi di cui tenere conto quando si utilizza l’algoritmo di massimizzazione delle aspettative.\n",
        "\n",
        "### Il risultato globalmente ottimale potrebbe non essere raggiunto\n",
        "In primo luogo, sebbene sia garantito che la procedura E-M migliori il risultato in ogni fase, non vi è alcuna garanzia che porti alla migliore soluzione globale. Ad esempio, se utilizziamo un seed casuale diverso nella nostra semplice procedura, le particolari ipotesi iniziali portano a scarsi risultati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UKrHmQcG88C",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "60b47e3d-1b05-48f4-9aae-6bf5cd606470"
      },
      "outputs": [],
      "source": [
        "centers, labels = find_clusters(X, 4, rseed=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GviCFjx7G88C"
      },
      "source": [
        "Qui l’approccio E-M è convergente, ma non verso una configurazione ottimale a livello globale. Per questo motivo, è normale che l'algoritmo venga eseguito per più ipotesi iniziali, come in effetti fa Scikit-Learn per impostazione predefinita (impostato dal n_init parametro, che per impostazione predefinita è 10)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGiZqmgG88C"
      },
      "source": [
        "#### Il numero di cluster deve essere selezionato in anticipo\n",
        "Un'altra sfida comune con k -means è che dobbiamo dirgli quanti cluster ci aspettiamo in anticipo: l'algoritmo infatti non può apprendere il numero di cluster dai dati. \n",
        "Ad esempio, se chiediamo all'algoritmo di identificare sei cluster, procederà felicemente e troverà i sei cluster migliori:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0HoIfWjG88C",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "7a6c58c8-4d4a-4bf7-cefb-99928934d775"
      },
      "outputs": [],
      "source": [
        "labels = KMeans(6, random_state=0).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzLde1A7G88C"
      },
      "source": [
        "Purtroppo se il risultato sia significativo è una domanda a cui è difficile rispondere in modo definitivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9k4Og-7G88C"
      },
      "source": [
        "#### k-medie è limitato ai confini dei cluster lineari\n",
        "I presupposti fondamentali del modello k -means (i punti saranno più vicini al centro del proprio cluster rispetto ad altri) significano che l'algoritmo sarà spesso inefficace se i cluster hanno geometrie complicate.\n",
        "\n",
        "In particolare, i confini tra i cluster k -medie saranno sempre lineari, il che significa che fallirà per confini più complicati. Consideriamo i seguenti dati, insieme alle etichette dei cluster trovate dal tipico approccio k -means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ4_IUgtG88D",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(200, noise=.05, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QroZBQV7G88D",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5c046a89-a3c3-4cb0-f141-babab75ea7c7"
      },
      "outputs": [],
      "source": [
        "labels = KMeans(2, random_state=0).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2oGrYdKG88D"
      },
      "source": [
        "Questa situazione ricorda l'approccio fatto con le SVM, dove abbiamo utilizzato una trasformazione del kernel per proiettare i dati in una dimensione superiore dove è possibile una separazione lineare. Potremmo immaginare di utilizzare lo stesso trucco per consentire a k-means di scoprire confini non lineari.\n",
        "\n",
        "Una versione di questo k-means kernelizzato è implementata in Scikit-Learn all'interno dello stimatore SpectralClustering , utilizziamo l'affinità dei KNN per calcolare una rappresentazione dimensionale superiore dei dati, quindi assegniamo le etichette utilizzando un algoritmo k-means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_VKDtS-G88D",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "151df2f2-7ab1-412d-ad62-0260f4f726cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
        "                           assign_labels='kmeans')\n",
        "labels = model.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZs76KsbG88D"
      },
      "source": [
        "Vediamo che con questo approccio di trasformazione del kernel, le k -mean kernelizzate sono in grado di trovare i confini non lineari più complicati tra i cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL_ZVV6KG88E"
      },
      "source": [
        "#### k-medie può essere lenta per un gran numero di campioni\n",
        "Poiché ogni iterazione di k-means deve accedere a ogni punto del set di dati, l'algoritmo può essere relativamente lento all'aumentare del numero di campioni. Una soluzione potrebbe essere non utilizzare tutti i dati ad ogni iterazione, ad esempio, potremo semplicemente utilizzare un sottoinsieme di dati per aggiornare i centri cluster a ogni passaggio. \n",
        "Questa è l'idea alla base degli algoritmi k-means basati su batch, una forma dei quali è implementata in sklearn.cluster.MiniBatchKMeans. L'interfaccia è la stessa di quella standard KMeans, vedremo un esempio del suo utilizzo più in la negli esempi più complessi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veX3TU-9G88E"
      },
      "source": [
        "## Esempi\n",
        "\n",
        "Facendo attenzione a queste limitazioni dell'algoritmo, possiamo utilizzare k -means a nostro vantaggio in un'ampia varietà di situazioni. Ora daremo un'occhiata ad un paio di esempi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fSYXu1kG88E"
      },
      "source": [
        "### Esempio 1: k-Means sulle cifre\n",
        "\n",
        "Quindi andiamo a vedere nella pratica l'esempio sul dataset delle cifre già visto in passato, con questo nuovo algoritmo. Qui tenteremo di utilizzare k -means per cercare di identificare cifre simili senza utilizzare le informazioni dell'etichetta originale, questo potrebbe essere simile a un primo passo per estrarre significato da un nuovo set di dati su cui non si dispone di informazioni sull'etichetta a priori .\n",
        "\n",
        "Inizieremo caricando le cifre e quindi trovando i KMeanscluster. Ricordiamo che le cifre sono costituite da 1.797 campioni con 64 caratteristiche, dove ciascuna delle 64 caratteristiche è la luminosità di un pixel in un'immagine 8×8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg9neItGG88E",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5e72c644-037e-414d-8e74-23eac55491d4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKBtoWasG88E"
      },
      "source": [
        "Il clustering può essere eseguito come abbiamo fatto prima:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JperxgNG88E",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "710a8bc4-7e3d-4577-d202-addc88d634b1"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits.data)\n",
        "kmeans.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxXQrm6fG88E"
      },
      "source": [
        "Il risultato sono 10 cluster in 64 dimensioni. Si noti che i centri stessi del cluster sono punti a 64 dimensioni e possono essere interpretati come la cifra \"tipica\" all'interno del cluster. Vediamo come appaiono questi centri cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNXe2WzwG88E",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "c6703537-0f61-49e0-d0b2-62f1e498905a"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
        "for axi, center in zip(ax.flat, centers):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKaoon9MG88F"
      },
      "source": [
        "Vediamo che anche senza le etichette , K-Means è in grado di trovare gruppi i cui centri sono cifre riconoscibili, forse con l'eccezione di 1 e 8.\n",
        "\n",
        "Poiché k -Means non sa nulla dell'identità del cluster, le etichette 0–9 possono essere permutate. Possiamo risolvere questo problema abbinando ciascuna etichetta del cluster appresa con le vere etichette trovate in essi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8KOlFG5G88I",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQf6CE3QG88I"
      },
      "source": [
        "Ora possiamo verificare quanto è stato accurato il nostro clustering non supervisionato nel trovare cifre simili all'interno dei dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAJRpdpxG88I",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "c37d063c-d1b6-4255-8eab-12de7c9b6758"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(digits.target, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3_rP-KMG88I"
      },
      "source": [
        "Con un semplice algoritmo k -means, abbiamo scoperto il raggruppamento corretto per il 75% delle cifre immesse! Controlliamo la matrice di confusione per questo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQqQw5AdG88I",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ba408342-583c-4e6e-f467-798c239cd7ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues',\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11YDYwlVG88J"
      },
      "source": [
        "Come potremmo aspettarci dai centri dei cluster che abbiamo visualizzato prima, il principale punto di confusione è tra gli otto, gli uno e i 9. Ma questo mostra ancora che usando k -means, possiamo essenzialmente costruire un classificatore di cifre senza riferimento ad alcuna etichetta conosciuta !\n",
        "\n",
        "Solo per divertimento, proviamo a spingerci ancora più lontano. Possiamo utilizzare l'algoritmo t-distributed stochastic neighbor embedding (t-SNE) per pre-elaborare i dati prima di eseguire k -mean. t-SNE è un algoritmo molto tecnico di cui non parleremo nel dettaglio per l'incorporamento non lineare particolarmente adatto a preservare punti all'interno dei cluster. Vediamo come si comporta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u--FaIXWG88J",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "537c134d-da42-435b-bbf5-0c2d9573d7ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Project the data: this step will take several seconds\n",
        "tsne = TSNE(n_components=2, init='random',\n",
        "            learning_rate='auto',random_state=0)\n",
        "digits_proj = tsne.fit_transform(digits.data)\n",
        "\n",
        "# Compute the clusters\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits_proj)\n",
        "\n",
        "# Permute the labels\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy_score(digits.target, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JabkSHFnG88J"
      },
      "source": [
        "Si tratta di una precisione di classificazione pari a quasi il 95% senza l'utilizzo delle etichette . Questo è il potere dell’apprendimento non supervisionato se usato con attenzione: può estrarre informazioni dal set di dati che potrebbe essere difficile da ottenere a mano o a occhio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuCtP7EAG88J"
      },
      "source": [
        "### Esempio 2: k-Means per la compressione del colore\n",
        "\n",
        "Un'applicazione interessante del clustering è la compressione del colore all'interno delle immagini. Ad esempio, immaginiamo di avere un'immagine con milioni di colori. Nella maggior parte delle immagini, un gran numero di colori rimarranno inutilizzati e molti pixel nell'immagine avranno colori simili o addirittura identici.\n",
        "\n",
        "Ad esempio, consideriamo l'immagine mostrata nella figura seguente, che proviene dal datasets del modulo Scikit-Learn (perché funzioni, dovrete avere installato il pacchetto Python pillow)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2t7hIOtG88J",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "03901c5e-5e5f-4bc9-d0ee-210d3c63c58d"
      },
      "outputs": [],
      "source": [
        "# Note: this requires the PIL package to be installed\n",
        "from sklearn.datasets import load_sample_image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "ax = plt.axes(xticks=[], yticks=[])\n",
        "ax.imshow(china)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSXwin4G88J"
      },
      "source": [
        "L'immagine stessa è memorizzata in un array tridimensionale di size (height, width, RGB), contenente gli attributi rosso/blu/verde come numeri interi da 0 a 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egSuRRjTG88J",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "96ac0030-6128-4b17-f115-5281f77ac016"
      },
      "outputs": [],
      "source": [
        "china.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSKx4HM5G88J"
      },
      "source": [
        "Un modo in cui possiamo visualizzare questo insieme di pixel è come una nuvola di punti in uno spazio colore tridimensionale. Rimodelleremo i dati in [n_samples x n_features] e ridimensioneremo i colori in modo che siano compresi tra 0 e 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN30AZaKG88J",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "8eb365cd-8b88-40eb-faf4-5aefe76a6512"
      },
      "outputs": [],
      "source": [
        "data = china / 255.0  # use 0...1 scale\n",
        "data = data.reshape(-1, 3)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByzAGBs-G88J"
      },
      "source": [
        "Possiamo visualizzare questi pixel in questo spazio colore, utilizzando un sottoinsieme di 10.000 pixel per efficienza:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-T5A0XWG88K",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def plot_pixels(data, title, colors=None, N=10000):\n",
        "    if colors is None:\n",
        "        colors = data\n",
        "\n",
        "    # choose a random subset\n",
        "    rng = np.random.default_rng(0)\n",
        "    i = rng.permutation(data.shape[0])[:N]\n",
        "    colors = colors[i]\n",
        "    R, G, B = data[i].T\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    ax[0].scatter(R, G, color=colors, marker='.')\n",
        "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
        "\n",
        "    ax[1].scatter(R, B, color=colors, marker='.')\n",
        "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
        "\n",
        "    fig.suptitle(title, size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgUHQpOG88K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "ca12b4a9-c5d6-457d-f263-e8487f96ab12"
      },
      "outputs": [],
      "source": [
        "plot_pixels(data, title='Input color space: 16 million possible colors')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ETkNbuuG88K"
      },
      "source": [
        "Ora riduciamo questi 16 milioni di colori a soli 16 colori, utilizzando un k-means clustering nello spazio dei pixel. Poiché abbiamo a che fare con un set di dati molto grande, utilizzeremo il mini batch k-means, che opera su sottoinsiemi di dati per calcolare il risultato molto più rapidamente rispetto all'algoritmo k-means standard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdoifLjoG88K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "bdb62add-4e30-42a0-e86d-d13170cdecc5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = MiniBatchKMeans(16)\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
        "\n",
        "plot_pixels(data, colors=new_colors,\n",
        "            title=\"Reduced color space: 16 colors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F51LufFsG88K"
      },
      "source": [
        "Il risultato è una ricolorazione dei pixel originali, in cui a ciascun pixel viene assegnato il colore del centro del cluster più vicino. Tracciare questi nuovi colori nello spazio dell'immagine anziché nello spazio dei pixel ci mostra l'effetto di questo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4B33ftxG88K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5b6fb8b0-8858-4e6b-e25a-dd8e673cea70"
      },
      "outputs": [],
      "source": [
        "china_recolored = new_colors.reshape(china.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(china)\n",
        "ax[0].set_title('Original Image', size=16)\n",
        "ax[1].imshow(china_recolored)\n",
        "ax[1].set_title('16-color Image', size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Z9pjMxG88K"
      },
      "source": [
        "Sicuramente si perde qualche dettaglio nel pannello più a destra, ma l'immagine complessiva è ancora facilmente riconoscibile. Questa immagine a destra raggiunge un fattore di compressione di circa 1 milione! Sebbene questa sia un'interessante applicazione di k-means, esiste sicuramente un modo migliore per comprimere le informazioni nelle immagini. Ma l’esempio mostra il potere di pensare fuori dagli schemi con metodi non supervisionati come k-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vediamo per curiosità lo stesso esempio con 16000 colori invece che 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = MiniBatchKMeans(16000)\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
        "\n",
        "plot_pixels(data, colors=new_colors,\n",
        "            title=\"Reduced color space: 16000 colors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "china_recolored = new_colors.reshape(china.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(china)\n",
        "ax[0].set_title('Original Image', size=16)\n",
        "ax[1].imshow(china_recolored)\n",
        "ax[1].set_title('16000-color Image', size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Come possiamo vedere il fattore di compressione rimane molto alto, ma in questo caso non abbiamo la perdita di qualità. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Esercizio 1 \n",
        "\n",
        "Obiettivo: Utilizzare l'algoritmo K-Means per raggruppare i dati di un dataset in cluster omogenei.\n",
        "\n",
        "Dataset: Utilizzeremo il famoso dataset Iris, che contiene informazioni su diverse specie di fiori Iris. Il dataset contiene quattro attributi (lunghezza e larghezza del sepalo, lunghezza e larghezza del petalo) e una variabile di classe che rappresenta la specie di fiore.\n",
        "\n",
        "Cosa dobbiamo fare:\n",
        "\n",
        "- Caricare il dataset Iris utilizzando la libreria scikit-learn.\n",
        "- Creare il clustering dei dati utilizzando l'algoritmo K-Means.\n",
        "- Determinare il numero ottimale di cluster utilizzando il metodo del gomito.\n",
        "- Visualizzare i cluster ottenuti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Esercizio 2\n",
        "\n",
        "Obiettivo: Utilizzare l'algoritmo K-Means per raggruppare i dati di un dataset in cluster omogenei.\n",
        "\n",
        "Dataset: Utilizzeremo il dataset \"Mall Customer Segmentation Data\" (scaricabile a questo link https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python) che contiene informazioni sui clienti di un centro commerciale, per il nostro esercizio utilizzeremo l'Annual Income e lo Spending Score.\n",
        "\n",
        "Cosa dobbiamo fare:\n",
        "\n",
        "- Caricare il dataset.\n",
        "- Creare il clustering dei dati utilizzando l'algoritmo K-Means.\n",
        "- Determinare il numero ottimale di cluster utilizzando il metodo del gomito.\n",
        "- Visualizzare i cluster ottenuti.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
